W&B online run started. URL: https://wandb.ai/ttl/tpu-vit/runs/14yllapi
Mixup is activated!
WARNING:timm.models._builder:No pretrained configuration specified for my_vit_b model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Forcing XLA synchronization after model.to(device)...
XLA mark_step() after model.to(device) completed
Forcing XLA synchronization before parameter counting...
XLA mark_step() completed
TPU spawned process: world_size = 32
Calculated total_batch_size = 4096
Number of training examples = 1281167
LR = 0.00400000
Batch size = 4096
Update frequent = 1
Number of training steps per epoch = 312
TPU multihost mode: XLA handles data distribution internally
About to create optimizer...
TPU mode: Using name-only parameter grouping to avoid XLA compilation
TPU mode: Collecting parameter names...
TPU mode: Found 152 trainable parameters
TPU mode: Grouping parameters by names...
Processing parameter 1/152: cls_token
Processing parameter 51/152: blocks.3.mlp.fc2.weight
Processing parameter 101/152: blocks.8.norm1.weight
Processing parameter 151/152: head.weight
TPU mode: Parameter names grouped successfully
TPU mode: Mapping names to tensors...
TPU mode: All 152 parameters mapped successfully
Parameter iteration completed! Processed 152 parameters.
TPU mode: Created 2 parameter groups, returning optimizer groups...
Parameter groups created successfully
Optimizer created successfully
TPU mode: Skipping loss scaler (XLA handles mixed precision)
TPU mode: About to set loss_scaler = None...
TPU mode: Loss scaler set to None
About to create Cosine LR scheduler...
Set warmup steps = 6240
LR scheduler created successfully
About to continue to weight decay scheduler...
About to check args.weight_decay_end...
Directly setting weight_decay_end to avoid freeze...
weight_decay_end set to: 0.05
About to call cosine_scheduler for weight decay...
Calling with: wd=0.05, wd_end=0.05, epochs=100, steps_per_epoch=312
Set warmup steps = 0
cosine_scheduler completed, about to calculate min/max...
Weight decay schedule created: 0.05 -> 0.05
About to create criterion...
Created SoftTargetCrossEntropy criterion
criterion = SoftTargetCrossEntropy()
TPU mode: Skipping warmup (caused shape errors), will compile on first iteration
About to call auto_load_model...
auto_load_model completed
About to define get_eval_model helper function...
get_eval_model function defined
About to set max_accuracy...
max_accuracy initialized
Start training for 100 epochs
About to record start_time...
About to enter training loop...
Starting epoch 0...
Setting sampler epoch...
Setting wandb steps...
About to call train_one_epoch...
train_one_epoch started: epoch=0, tpu=True
About to zero_grad optimizer...
About to start data loader loop...
Starting data_iter_step 0...
Calculated step = 0, num_training_steps_per_epoch = 312
Global iteration it = 0
TPU mode: Processing iteration 0, step 0, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 0...
TPU mode: Current device = xla:0, device type = xla
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
TPU mode: samples.device = xla:0
TPU mode: targets.device = xla:0
TPU mode: model device = xla:0
About to call model forward...
✅ Model forward completed in 0.020s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.023s
About to run optimizer step...
✅ Optimizer step completed in 26.553s
🔥 Total XLA step time: 38.582s
TPU mode: Iteration 0 completed in 38.59s
📝 End of iteration 0 - about to continue loop...
TPU mode: Iteration 0, TPU memory: {'bytes_used': 1119007744, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 18.742s
✅ Wandb logged in 0.000s
Epoch: [0]  [  0/312]  eta: 5:01:51  loss: 7.0625 (7.0625)  lr: 0.0000 (0.0000)  time: 58.0499  data: 0.7199
Starting data_iter_step 1...
Calculated step = 1, num_training_steps_per_epoch = 312
Global iteration it = 1
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 1...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 7.125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 28.706s
🔥 Total XLA step time: 28.800s
TPU mode: Iteration 1 completed in 28.84s
📝 End of iteration 1 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 11.890s
✅ Wandb logged in 0.000s
Starting data_iter_step 2...
Calculated step = 2, num_training_steps_per_epoch = 312
Global iteration it = 2
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 2...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.185s
🔥 Total XLA step time: 0.233s
TPU mode: Iteration 2 completed in 0.24s
📝 End of iteration 2 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.138s
✅ Wandb logged in 0.000s
Starting data_iter_step 3...
Calculated step = 3, num_training_steps_per_epoch = 312
Global iteration it = 3
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 3...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.027s
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.273s
TPU mode: Iteration 3 completed in 0.41s
📝 End of iteration 3 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.115s
✅ Wandb logged in 0.000s
Starting data_iter_step 4...
Calculated step = 4, num_training_steps_per_epoch = 312
Global iteration it = 4
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 4...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.029s
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.187s
🔥 Total XLA step time: 0.274s
TPU mode: Iteration 4 completed in 0.42s
📝 End of iteration 4 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.342s
✅ Wandb logged in 0.000s
Starting data_iter_step 5...
Calculated step = 5, num_training_steps_per_epoch = 312
Global iteration it = 5
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 5...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.234s
🔧 Starting metric updates...
✅ Metric logger updated in 0.215s
✅ Wandb logged in 0.000s
Starting data_iter_step 6...
Calculated step = 6, num_training_steps_per_epoch = 312
Global iteration it = 6
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 6...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.039s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.187s
🔥 Total XLA step time: 0.288s
🔧 Starting metric updates...
✅ Metric logger updated in 0.179s
✅ Wandb logged in 0.000s
Starting data_iter_step 7...
Calculated step = 7, num_training_steps_per_epoch = 312
Global iteration it = 7
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 7...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.020s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.261s
🔧 Starting metric updates...
✅ Metric logger updated in 0.435s
✅ Wandb logged in 0.000s
Starting data_iter_step 8...
Calculated step = 8, num_training_steps_per_epoch = 312
Global iteration it = 8
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 8...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.185s
🔥 Total XLA step time: 0.234s
🔧 Starting metric updates...
✅ Metric logger updated in 0.109s
✅ Wandb logged in 0.000s
Starting data_iter_step 9...
Calculated step = 9, num_training_steps_per_epoch = 312
Global iteration it = 9
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 9...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.021s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.255s
🔧 Starting metric updates...
✅ Metric logger updated in 0.295s
✅ Wandb logged in 0.000s
Starting data_iter_step 10...
Calculated step = 10, num_training_steps_per_epoch = 312
Global iteration it = 10
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 10...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.022s
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.000s: 7.09375, about to backward...
✅ Backward completed in 0.018s
About to run optimizer step...
✅ Optimizer step completed in 0.195s
🔥 Total XLA step time: 0.280s
🔧 Starting metric updates...
✅ Metric logger updated in 0.153s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 10/312]  eta: 1:58:56  loss: 7.0625 (7.0710)  lr: 0.0000 (0.0000)  time: 23.6317  data: 14.2506
Starting data_iter_step 11...
Calculated step = 11, num_training_steps_per_epoch = 312
Global iteration it = 11
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 11...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.022s
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.260s
🔧 Starting metric updates...
✅ Metric logger updated in 0.442s
✅ Wandb logged in 0.000s
