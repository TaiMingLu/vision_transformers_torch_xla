W&B online run started. URL: https://wandb.ai/ttl/tpu-vit/runs/j5aap36c
Mixup is activated!
WARNING:timm.models._builder:No pretrained configuration specified for my_vit_b model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Forcing XLA synchronization after model.to(device)...
XLA mark_step() after model.to(device) completed
Using EMA with decay = 0.99990000
Forcing XLA synchronization before parameter counting...
XLA mark_step() completed
TPU spawned process: world_size = 32
Calculated total_batch_size = 2048
Number of training examples = 1281167
LR = 0.00400000
Batch size = 2048
Update frequent = 1
Number of training steps per epoch = 625
TPU multihost mode: XLA handles data distribution internally
About to create optimizer...
TPU mode: About to call xm.mark_step() before parameter iteration...
TPU mode: xm.mark_step() completed
TPU mode: Using name-only parameter grouping to avoid XLA compilation
TPU mode: Collecting parameter names...
TPU mode: Found 152 trainable parameters
TPU mode: Grouping parameters by names...
Processing parameter 1/152: cls_token
Processing parameter 51/152: blocks.3.mlp.fc2.weight
Processing parameter 101/152: blocks.8.norm1.weight
Processing parameter 151/152: head.weight
TPU mode: Parameter names grouped successfully
TPU mode: Mapping names to tensors...
TPU mode: All 152 parameters mapped successfully
Parameter iteration completed! Processed 152 parameters.
TPU mode: Final XLA synchronization...
TPU mode: Final mark_step() completed
About to print param groups...
TPU mode: Skipping detailed param group printing to avoid XLA issues
Created 2 parameter groups
  - no_decay: 102 parameters
  - decay: 50 parameters
About to return parameter groups...
Parameter groups created successfully
Optimizer created successfully
TPU mode: Forcing XLA sync after optimizer creation...
TPU mode: XLA sync completed
TPU mode: Skipping loss scaler (XLA handles mixed precision)
TPU mode: About to set loss_scaler = None...
TPU mode: Loss scaler set to None
About to create Cosine LR scheduler...
Set warmup steps = 12500
LR scheduler created successfully
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 100 epochs
Epoch: [0]  [  0/625]  eta: 11:15:39  loss: 7.0625 (7.0625)  lr: 0.0000 (0.0000)  time: 64.8636  data: 0.5238
Epoch: [0]  [ 10/625]  eta: 3:48:21  loss: 7.0625 (7.0653)  lr: 0.0000 (0.0000)  time: 22.2782  data: 5.2019
Epoch: [0]  [ 20/625]  eta: 2:42:39  loss: 7.0625 (7.0670)  lr: 0.0000 (0.0000)  time: 13.6949  data: 7.1194
Epoch: [0]  [ 30/625]  eta: 2:17:19  loss: 7.0625 (7.0746)  lr: 0.0000 (0.0000)  time: 9.2113  data: 8.5219
Epoch: [0]  [ 40/625]  eta: 2:02:08  loss: 7.0625 (7.0663)  lr: 0.0000 (0.0000)  time: 8.7432  data: 8.1559
Epoch: [0]  [ 50/625]  eta: 1:53:51  loss: 7.0625 (7.0668)  lr: 0.0000 (0.0000)  time: 8.8330  data: 8.2699
Epoch: [0]  [ 60/625]  eta: 1:47:20  loss: 7.0625 (7.0625)  lr: 0.0000 (0.0000)  time: 9.0875  data: 8.5488
Epoch: [0]  [ 70/625]  eta: 1:41:34  loss: 7.0625 (7.0603)  lr: 0.0000 (0.0000)  time: 8.6877  data: 7.5497
