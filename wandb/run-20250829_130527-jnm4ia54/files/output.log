W&B online run started. URL: https://wandb.ai/ttl/tpu-vit/runs/jnm4ia54
TPU mode: Using num_workers=0 to avoid pickle issues, will use MpDeviceLoader for parallelism
Mixup is activated!
WARNING:timm.models._builder:No pretrained configuration specified for my_vit_b model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Forcing XLA synchronization after model.to(device)...
XLA mark_step() after model.to(device) completed
Forcing XLA synchronization before parameter counting...
XLA mark_step() completed
TPU spawned process: world_size = 64
Calculated total_batch_size = 16384
Number of training examples = 1281167
LR = 0.00400000
Batch size = 16384
Update frequent = 1
Number of training steps per epoch = 78
TPU multihost mode: XLA handles data distribution internally
About to create optimizer...
TPU mode: Using name-only parameter grouping to avoid XLA compilation
TPU mode: Collecting parameter names...
TPU mode: Found 152 trainable parameters
TPU mode: Grouping parameters by names...
Processing parameter 1/152: cls_token
Processing parameter 51/152: blocks.3.mlp.fc2.weight
Processing parameter 101/152: blocks.8.norm1.weight
Processing parameter 151/152: head.weight
TPU mode: Parameter names grouped successfully
TPU mode: Mapping names to tensors...
TPU mode: All 152 parameters mapped successfully
Parameter iteration completed! Processed 152 parameters.
TPU mode: Created 2 parameter groups, returning optimizer groups...
Parameter groups created successfully
Optimizer created successfully
TPU mode: Skipping loss scaler (XLA handles mixed precision)
TPU mode: About to set loss_scaler = None...
TPU mode: Loss scaler set to None
About to create Cosine LR scheduler...
Set warmup steps = 1560
LR scheduler created successfully
About to continue to weight decay scheduler...
About to check args.weight_decay_end...
Directly setting weight_decay_end to avoid freeze...
weight_decay_end set to: 0.05
About to call cosine_scheduler for weight decay...
Calling with: wd=0.05, wd_end=0.05, epochs=100, steps_per_epoch=78
Set warmup steps = 0
cosine_scheduler completed, about to calculate min/max...
Weight decay schedule created: 0.05 -> 0.05
About to create criterion...
Created SoftTargetCrossEntropy criterion
criterion = SoftTargetCrossEntropy()
TPU mode: Skipping warmup (caused shape errors), will compile on first iteration
About to call auto_load_model...
auto_load_model completed
About to define get_eval_model helper function...
get_eval_model function defined
About to set max_accuracy...
max_accuracy initialized
ğŸš€ Creating MpDeviceLoader for training DataLoader (TPU optimization)...
âœ… Training MpDeviceLoader created successfully
ğŸš€ Creating MpDeviceLoader for validation DataLoader (TPU optimization)...
âœ… Validation MpDeviceLoader created successfully
Start training for 100 epochs
About to record start_time...
About to enter training loop...
Starting epoch 0...
Setting sampler epoch...
Setting wandb steps...
About to call train_one_epoch...
About to zero_grad optimizer...
About to start data loader loop...
ğŸš€ Iteration: 0
Epoch: [0]  [ 0/78]  eta: 5:56:39  loss: 7.0938 (7.0938)  lr: 0.0000 (0.0000)  time: 274.3475  data: 235.2231
ğŸš€ Iteration: 1
ğŸš€ Iteration: 2
ğŸš€ Iteration: 3
ğŸš€ Iteration: 4
ğŸš€ Iteration: 5
ğŸš€ Iteration: 6
ğŸš€ Iteration: 7
ğŸš€ Iteration: 8
ğŸš€ Iteration: 9
ğŸš€ Iteration: 10
Epoch: [0]  [10/78]  eta: 0:32:55  loss: 7.0625 (7.0511)  lr: 0.0000 (0.0000)  time: 29.0502  data: 21.3844
ğŸš€ Iteration: 11
ğŸš€ Iteration: 12
ğŸš€ Iteration: 13
ğŸš€ Iteration: 14
ğŸš€ Iteration: 15
ğŸš€ Iteration: 16
ğŸš€ Iteration: 17
ğŸš€ Iteration: 18
ğŸš€ Iteration: 19
ğŸš€ Iteration: 20
Epoch: [0]  [20/78]  eta: 0:15:07  loss: 7.0312 (7.0446)  lr: 0.0000 (0.0000)  time: 2.7189  data: 0.0005
ğŸš€ Iteration: 21
ğŸš€ Iteration: 22
ğŸš€ Iteration: 23
ğŸš€ Iteration: 24
ğŸš€ Iteration: 25
ğŸš€ Iteration: 26
ğŸš€ Iteration: 27
ğŸš€ Iteration: 28
ğŸš€ Iteration: 29
ğŸš€ Iteration: 30
Epoch: [0]  [30/78]  eta: 0:08:43  loss: 7.0312 (7.0373)  lr: 0.0001 (0.0000)  time: 0.9127  data: 0.0005
ğŸš€ Iteration: 31
ğŸš€ Iteration: 32
ğŸš€ Iteration: 33
ğŸš€ Iteration: 34
ğŸš€ Iteration: 35
ğŸš€ Iteration: 36
ğŸš€ Iteration: 37
ğŸš€ Iteration: 38
ğŸš€ Iteration: 39
ğŸš€ Iteration: 40
Epoch: [0]  [40/78]  eta: 0:05:21  loss: 7.0000 (7.0274)  lr: 0.0001 (0.0001)  time: 0.8986  data: 0.0005
ğŸš€ Iteration: 41
ğŸš€ Iteration: 42
ğŸš€ Iteration: 43
ğŸš€ Iteration: 44
ğŸš€ Iteration: 45
ğŸš€ Iteration: 46
ğŸš€ Iteration: 47
ğŸš€ Iteration: 48
ğŸš€ Iteration: 49
ğŸš€ Iteration: 50
Epoch: [0]  [50/78]  eta: 0:03:15  loss: 7.0000 (7.0214)  lr: 0.0001 (0.0001)  time: 0.9001  data: 0.0005
ğŸš€ Iteration: 51
ğŸš€ Iteration: 52
ğŸš€ Iteration: 53
ğŸš€ Iteration: 54
ğŸš€ Iteration: 55
ğŸš€ Iteration: 56
ğŸš€ Iteration: 57
ğŸš€ Iteration: 58
ğŸš€ Iteration: 59
ğŸš€ Iteration: 60
Epoch: [0]  [60/78]  eta: 0:01:49  loss: 7.0000 (7.0149)  lr: 0.0001 (0.0001)  time: 1.1678  data: 0.2189
ğŸš€ Iteration: 61
ğŸš€ Iteration: 62
ğŸš€ Iteration: 63
ğŸš€ Iteration: 64
ğŸš€ Iteration: 65
ğŸš€ Iteration: 66
ğŸš€ Iteration: 67
ğŸš€ Iteration: 68
ğŸš€ Iteration: 69
ğŸš€ Iteration: 70
Epoch: [0]  [70/78]  eta: 0:00:43  loss: 6.9688 (7.0084)  lr: 0.0002 (0.0001)  time: 1.4288  data: 0.4523
ğŸš€ Iteration: 71
ğŸš€ Iteration: 72
ğŸš€ Iteration: 73
ğŸš€ Iteration: 74
ğŸš€ Iteration: 75
ğŸš€ Iteration: 76
ğŸš€ Iteration: 77
Epoch: [0]  [77/78]  eta: 0:00:05  loss: 6.9688 (7.0052)  lr: 0.0002 (0.0001)  time: 1.2517  data: 0.3144
Epoch: [0] Total time: 0:06:32 (5.0307 s / it)
Averaged stats: loss: 6.9688 (7.0256)  lr: 0.0002 (0.0001)
Done train_one_epoch...
Checking checkpoint save conditions...
About to start validation...
Getting eval model...
About to call evaluate()...
Starting validation loop...
Test:  [0/3]  eta: 0:05:02  loss: 6.9688 (6.9688)  acc1: 0.2598 (0.2598)  acc5: 1.0391 (1.0391)  time: 100.9706  data: 92.3195
Test:  [2/3]  eta: 0:00:36  loss: 6.9062 (6.9271)  acc1: 0.2598 (0.3827)  acc5: 1.0391 (1.0205)  time: 36.4742  data: 30.8194
Test: Total time: 0:01:49 (36.4897 s / it)
* Acc@1 0.181 Acc@5 0.847 loss 6.917
Done evaluate()...
Accuracy of the model on the 50000 test images: 0.2%
Max accuracy: 0.18%
Starting epoch 1...
Setting sampler epoch...
Setting wandb steps...
About to call train_one_epoch...
About to zero_grad optimizer...
About to start data loader loop...
ğŸš€ Iteration: 0
Epoch: [1]  [ 0/78]  eta: 5:26:00  loss: 6.9375 (6.9375)  lr: 0.0002 (0.0002)  time: 250.7719  data: 248.8472
ğŸš€ Iteration: 1
ğŸš€ Iteration: 2
ğŸš€ Iteration: 3
ğŸš€ Iteration: 4
ğŸš€ Iteration: 5
ğŸš€ Iteration: 6
ğŸš€ Iteration: 7
ğŸš€ Iteration: 8
ğŸš€ Iteration: 9
ğŸš€ Iteration: 10
Epoch: [1]  [10/78]  eta: 0:27:16  loss: 6.9688 (6.9631)  lr: 0.0002 (0.0002)  time: 24.0638  data: 22.9928
ğŸš€ Iteration: 11
ğŸš€ Iteration: 12
ğŸš€ Iteration: 13
ğŸš€ Iteration: 14
ğŸš€ Iteration: 15
ğŸš€ Iteration: 16
ğŸš€ Iteration: 17
ğŸš€ Iteration: 18
ğŸš€ Iteration: 19
ğŸš€ Iteration: 20
Epoch: [1]  [20/78]  eta: 0:12:51  loss: 6.9688 (6.9554)  lr: 0.0002 (0.0002)  time: 1.4206  data: 0.4353
ğŸš€ Iteration: 21
ğŸš€ Iteration: 22
ğŸš€ Iteration: 23
ğŸš€ Iteration: 24
ğŸš€ Iteration: 25
ğŸš€ Iteration: 26
ğŸš€ Iteration: 27
ğŸš€ Iteration: 28
ğŸš€ Iteration: 29
ğŸš€ Iteration: 30
Epoch: [1]  [30/78]  eta: 0:07:33  loss: 6.9375 (6.9506)  lr: 0.0003 (0.0002)  time: 1.4187  data: 0.4346
ğŸš€ Iteration: 31
ğŸš€ Iteration: 32
ğŸš€ Iteration: 33
ğŸš€ Iteration: 34
ğŸš€ Iteration: 35
ğŸš€ Iteration: 36
ğŸš€ Iteration: 37
ğŸš€ Iteration: 38
ğŸš€ Iteration: 39
ğŸš€ Iteration: 40
Epoch: [1]  [40/78]  eta: 0:04:50  loss: 6.9375 (6.9489)  lr: 0.0003 (0.0003)  time: 1.6889  data: 0.6280
ğŸš€ Iteration: 41
ğŸš€ Iteration: 42
ğŸš€ Iteration: 43
ğŸš€ Iteration: 44
ğŸš€ Iteration: 45
ğŸš€ Iteration: 46
ğŸš€ Iteration: 47
ğŸš€ Iteration: 48
ğŸš€ Iteration: 49
ğŸš€ Iteration: 50
Epoch: [1]  [50/78]  eta: 0:02:59  loss: 6.9375 (6.9449)  lr: 0.0003 (0.0003)  time: 1.7067  data: 0.6324
ğŸš€ Iteration: 51
ğŸš€ Iteration: 52
ğŸš€ Iteration: 53
ğŸš€ Iteration: 54
ğŸš€ Iteration: 55
ğŸš€ Iteration: 56
ğŸš€ Iteration: 57
ğŸš€ Iteration: 58
ğŸš€ Iteration: 59
ğŸš€ Iteration: 60
Epoch: [1]  [60/78]  eta: 0:01:40  loss: 6.9375 (6.9436)  lr: 0.0003 (0.0003)  time: 1.4095  data: 0.4108
ğŸš€ Iteration: 61
ğŸš€ Iteration: 62
ğŸš€ Iteration: 63
ğŸš€ Iteration: 64
ğŸš€ Iteration: 65
ğŸš€ Iteration: 66
ğŸš€ Iteration: 67
ğŸš€ Iteration: 68
ğŸš€ Iteration: 69
ğŸš€ Iteration: 70
Epoch: [1]  [70/78]  eta: 0:00:40  loss: 6.9375 (6.9410)  lr: 0.0004 (0.0003)  time: 1.4339  data: 0.4101
ğŸš€ Iteration: 71
ğŸš€ Iteration: 72
ğŸš€ Iteration: 73
ğŸš€ Iteration: 74
ğŸš€ Iteration: 75
ğŸš€ Iteration: 76
ğŸš€ Iteration: 77
Epoch: [1]  [77/78]  eta: 0:00:04  loss: 6.9375 (6.9395)  lr: 0.0004 (0.0003)  time: 1.2711  data: 0.2370
Epoch: [1] Total time: 0:06:03 (4.6649 s / it)
Averaged stats: loss: 6.9375 (6.9231)  lr: 0.0004 (0.0003)
Done train_one_epoch...
Checking checkpoint save conditions...
About to start validation...
Getting eval model...
About to call evaluate()...
Starting validation loop...
Test:  [0/3]  eta: 0:04:39  loss: 6.8750 (6.8750)  acc1: 0.2598 (0.2598)  acc5: 0.7812 (0.7812)  time: 93.1323  data: 93.0484
Test:  [2/3]  eta: 0:00:31  loss: 6.8750 (6.8958)  acc1: 0.2598 (0.2551)  acc5: 0.7812 (0.8939)  time: 31.0706  data: 31.0166
Test: Total time: 0:01:33 (31.0712 s / it)
* Acc@1 0.315 Acc@5 1.378 loss 6.792
Done evaluate()...
Accuracy of the model on the 50000 test images: 0.3%
Max accuracy: 0.32%
Starting epoch 2...
Setting sampler epoch...
Setting wandb steps...
About to call train_one_epoch...
About to zero_grad optimizer...
About to start data loader loop...
ğŸš€ Iteration: 0
Epoch: [2]  [ 0/78]  eta: 5:26:38  loss: 6.9688 (6.9688)  lr: 0.0004 (0.0004)  time: 251.2688  data: 246.9265
ğŸš€ Iteration: 1
