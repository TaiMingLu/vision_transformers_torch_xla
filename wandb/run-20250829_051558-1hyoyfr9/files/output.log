W&B online run started. URL: https://wandb.ai/ttl/tpu-vit/runs/1hyoyfr9
TPU mode: Using num_workers=0 to avoid pickle issues, will use MpDeviceLoader for parallelism
Mixup is activated!
WARNING:timm.models._builder:No pretrained configuration specified for my_vit_b model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Forcing XLA synchronization after model.to(device)...
XLA mark_step() after model.to(device) completed
Forcing XLA synchronization before parameter counting...
XLA mark_step() completed
TPU spawned process: world_size = 32
Calculated total_batch_size = 4096
Number of training examples = 1281167
LR = 0.00400000
Batch size = 4096
Update frequent = 1
Number of training steps per epoch = 312
TPU multihost mode: XLA handles data distribution internally
About to create optimizer...
TPU mode: Using name-only parameter grouping to avoid XLA compilation
TPU mode: Collecting parameter names...
TPU mode: Found 152 trainable parameters
TPU mode: Grouping parameters by names...
Processing parameter 1/152: cls_token
Processing parameter 51/152: blocks.3.mlp.fc2.weight
Processing parameter 101/152: blocks.8.norm1.weight
Processing parameter 151/152: head.weight
TPU mode: Parameter names grouped successfully
TPU mode: Mapping names to tensors...
TPU mode: All 152 parameters mapped successfully
Parameter iteration completed! Processed 152 parameters.
TPU mode: Created 2 parameter groups, returning optimizer groups...
Parameter groups created successfully
Optimizer created successfully
TPU mode: Skipping loss scaler (XLA handles mixed precision)
TPU mode: About to set loss_scaler = None...
TPU mode: Loss scaler set to None
About to create Cosine LR scheduler...
Set warmup steps = 6240
LR scheduler created successfully
About to continue to weight decay scheduler...
About to check args.weight_decay_end...
Directly setting weight_decay_end to avoid freeze...
weight_decay_end set to: 0.05
About to call cosine_scheduler for weight decay...
Calling with: wd=0.05, wd_end=0.05, epochs=100, steps_per_epoch=312
Set warmup steps = 0
cosine_scheduler completed, about to calculate min/max...
Weight decay schedule created: 0.05 -> 0.05
About to create criterion...
Created SoftTargetCrossEntropy criterion
criterion = SoftTargetCrossEntropy()
TPU mode: Skipping warmup (caused shape errors), will compile on first iteration
About to call auto_load_model...
auto_load_model completed
About to define get_eval_model helper function...
get_eval_model function defined
About to set max_accuracy...
max_accuracy initialized
Start training for 100 epochs
About to record start_time...
About to enter training loop...
Starting epoch 0...
Setting sampler epoch...
Setting wandb steps...
About to call train_one_epoch...
train_one_epoch started: epoch=0, tpu=True
🚀 Wrapping DataLoader with MpDeviceLoader for TPU parallelism...
✅ MpDeviceLoader created successfully - enables prefetching and parallelism
About to zero_grad optimizer...
About to start data loader loop...
🚀 Starting data_iter_step 0 (DataLoader iteration took 0.000s)...
Calculated step = 0, num_training_steps_per_epoch = 312
Global iteration it = 0
TPU mode: Processing iteration 0, step 0, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 0...
TPU mode: Current device = xla:0, device type = xla
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
TPU mode: samples.device = xla:0
TPU mode: targets.device = xla:0
TPU mode: model device = xla:0
About to call model forward...
✅ Model forward completed in 0.022s, about to compute loss...
✅ Loss computed in 0.000s: 7.09375, about to backward...
✅ Backward completed in 0.023s
About to run optimizer step...
✅ Optimizer step completed in 27.010s
🔥 Total XLA step time: 40.349s
TPU mode: Iteration 0 completed in 40.35s
📝 End of iteration 0 - about to continue loop...
TPU mode: Iteration 0, TPU memory: {'bytes_used': 1697257472, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 18.010s
✅ Wandb logged in 0.000s
Epoch: [0]  [  0/312]  eta: 14:02:50  loss: 7.0938 (7.0938)  lr: 0.0000 (0.0000)  time: 162.0838  data: 103.7159
🚀 Starting data_iter_step 1 (DataLoader iteration took 18.012s)...
Calculated step = 1, num_training_steps_per_epoch = 312
Global iteration it = 1
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 1...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 29.492s
🔥 Total XLA step time: 29.912s
TPU mode: Iteration 1 completed in 29.91s
📝 End of iteration 1 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 10.717s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 2 (DataLoader iteration took 10.718s)...
Calculated step = 2, num_training_steps_per_epoch = 312
Global iteration it = 2
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 2...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.232s
TPU mode: Iteration 2 completed in 0.23s
📝 End of iteration 2 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.519s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 3 (DataLoader iteration took 0.521s)...
Calculated step = 3, num_training_steps_per_epoch = 312
Global iteration it = 3
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 3...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.232s
TPU mode: Iteration 3 completed in 0.23s
📝 End of iteration 3 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.570s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 4 (DataLoader iteration took 0.571s)...
Calculated step = 4, num_training_steps_per_epoch = 312
Global iteration it = 4
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 4...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.232s
TPU mode: Iteration 4 completed in 0.23s
📝 End of iteration 4 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.557s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 5 (DataLoader iteration took 0.559s)...
Calculated step = 5, num_training_steps_per_epoch = 312
Global iteration it = 5
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 5...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.185s
🔥 Total XLA step time: 0.576s
TPU mode: Iteration 5 completed in 0.58s
📝 End of iteration 5 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.089s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 6 (DataLoader iteration took 0.090s)...
Calculated step = 6, num_training_steps_per_epoch = 312
Global iteration it = 6
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 6...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.550s
TPU mode: Iteration 6 completed in 0.55s
📝 End of iteration 6 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.226s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 7 (DataLoader iteration took 0.228s)...
Calculated step = 7, num_training_steps_per_epoch = 312
Global iteration it = 7
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 7...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.254s
🔥 Total XLA step time: 0.746s
TPU mode: Iteration 7 completed in 0.75s
📝 End of iteration 7 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.102s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 8 (DataLoader iteration took 0.103s)...
Calculated step = 8, num_training_steps_per_epoch = 312
Global iteration it = 8
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 8...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.725s
TPU mode: Iteration 8 completed in 0.73s
📝 End of iteration 8 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.135s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 9 (DataLoader iteration took 0.137s)...
Calculated step = 9, num_training_steps_per_epoch = 312
Global iteration it = 9
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 9...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.689s
TPU mode: Iteration 9 completed in 0.69s
📝 End of iteration 9 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.131s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 10 (DataLoader iteration took 0.132s)...
Calculated step = 10, num_training_steps_per_epoch = 312
Global iteration it = 10
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 10...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.288s
TPU mode: Iteration 10 completed in 0.29s
📝 End of iteration 10 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.570s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 10/312]  eta: 1:36:03  loss: 7.0625 (7.0568)  lr: 0.0000 (0.0000)  time: 19.0846  data: 9.4292
🚀 Starting data_iter_step 11 (DataLoader iteration took 0.572s)...
Calculated step = 11, num_training_steps_per_epoch = 312
Global iteration it = 11
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 11...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.667s
TPU mode: Iteration 11 completed in 0.67s
📝 End of iteration 11 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.416s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 12 (DataLoader iteration took 0.418s)...
Calculated step = 12, num_training_steps_per_epoch = 312
Global iteration it = 12
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 12...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.631s
TPU mode: Iteration 12 completed in 0.63s
📝 End of iteration 12 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.127s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 13 (DataLoader iteration took 0.128s)...
Calculated step = 13, num_training_steps_per_epoch = 312
Global iteration it = 13
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 13...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.289s
TPU mode: Iteration 13 completed in 0.29s
📝 End of iteration 13 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.576s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 14 (DataLoader iteration took 0.577s)...
Calculated step = 14, num_training_steps_per_epoch = 312
Global iteration it = 14
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 14...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.15625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.224s
🔥 Total XLA step time: 0.273s
TPU mode: Iteration 14 completed in 0.28s
📝 End of iteration 14 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.426s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 15 (DataLoader iteration took 0.431s)...
Calculated step = 15, num_training_steps_per_epoch = 312
Global iteration it = 15
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 15...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.628s
TPU mode: Iteration 15 completed in 0.64s
📝 End of iteration 15 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.246s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 16 (DataLoader iteration took 0.248s)...
Calculated step = 16, num_training_steps_per_epoch = 312
Global iteration it = 16
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 16...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.289s
TPU mode: Iteration 16 completed in 0.29s
📝 End of iteration 16 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.568s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 17 (DataLoader iteration took 0.570s)...
Calculated step = 17, num_training_steps_per_epoch = 312
Global iteration it = 17
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 17...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.259s
🔥 Total XLA step time: 0.309s
TPU mode: Iteration 17 completed in 0.31s
📝 End of iteration 17 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.542s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 18 (DataLoader iteration took 0.544s)...
Calculated step = 18, num_training_steps_per_epoch = 312
Global iteration it = 18
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 18...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.283s
TPU mode: Iteration 18 completed in 0.28s
📝 End of iteration 18 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.503s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 19 (DataLoader iteration took 0.505s)...
Calculated step = 19, num_training_steps_per_epoch = 312
Global iteration it = 19
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 19...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.223s
🔥 Total XLA step time: 0.627s
TPU mode: Iteration 19 completed in 0.63s
📝 End of iteration 19 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.194s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 20 (DataLoader iteration took 0.196s)...
Calculated step = 20, num_training_steps_per_epoch = 312
Global iteration it = 20
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 20...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.019s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.258s
🔥 Total XLA step time: 0.313s
🔧 Starting metric updates...
✅ Metric logger updated in 0.419s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 20/312]  eta: 0:50:35  loss: 7.0312 (7.0521)  lr: 0.0000 (0.0000)  time: 2.8117  data: 0.0005
🚀 Starting data_iter_step 21 (DataLoader iteration took 0.421s)...
Calculated step = 21, num_training_steps_per_epoch = 312
Global iteration it = 21
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 21...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.289s
🔧 Starting metric updates...
✅ Metric logger updated in 0.865s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 22 (DataLoader iteration took 0.866s)...
Calculated step = 22, num_training_steps_per_epoch = 312
Global iteration it = 22
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 22...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.292s
🔧 Starting metric updates...
✅ Metric logger updated in 0.463s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 23 (DataLoader iteration took 0.467s)...
Calculated step = 23, num_training_steps_per_epoch = 312
Global iteration it = 23
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 23...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.606s
🔧 Starting metric updates...
✅ Metric logger updated in 0.234s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 24 (DataLoader iteration took 0.237s)...
Calculated step = 24, num_training_steps_per_epoch = 312
Global iteration it = 24
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 24...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.677s
🔧 Starting metric updates...
✅ Metric logger updated in 0.115s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 25 (DataLoader iteration took 0.117s)...
Calculated step = 25, num_training_steps_per_epoch = 312
Global iteration it = 25
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 25...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.291s
🔧 Starting metric updates...
✅ Metric logger updated in 0.487s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 26 (DataLoader iteration took 0.488s)...
Calculated step = 26, num_training_steps_per_epoch = 312
Global iteration it = 26
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 26...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.287s
🔧 Starting metric updates...
✅ Metric logger updated in 0.573s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 27 (DataLoader iteration took 0.575s)...
Calculated step = 27, num_training_steps_per_epoch = 312
Global iteration it = 27
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 27...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.286s
🔧 Starting metric updates...
✅ Metric logger updated in 0.457s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 28 (DataLoader iteration took 0.458s)...
Calculated step = 28, num_training_steps_per_epoch = 312
Global iteration it = 28
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 28...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.645s
🔧 Starting metric updates...
✅ Metric logger updated in 0.110s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 29 (DataLoader iteration took 0.111s)...
Calculated step = 29, num_training_steps_per_epoch = 312
Global iteration it = 29
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 29...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.636s
🔧 Starting metric updates...
✅ Metric logger updated in 0.263s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 30 (DataLoader iteration took 0.265s)...
Calculated step = 30, num_training_steps_per_epoch = 312
Global iteration it = 30
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 30...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.289s
🔧 Starting metric updates...
✅ Metric logger updated in 0.552s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 30/312]  eta: 0:34:23  loss: 7.0312 (7.0423)  lr: 0.0000 (0.0000)  time: 0.8429  data: 0.0006
🚀 Starting data_iter_step 31 (DataLoader iteration took 0.557s)...
Calculated step = 31, num_training_steps_per_epoch = 312
Global iteration it = 31
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 31...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.637s
🔧 Starting metric updates...
✅ Metric logger updated in 0.210s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 32 (DataLoader iteration took 0.213s)...
Calculated step = 32, num_training_steps_per_epoch = 312
Global iteration it = 32
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 32...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.296s
🔧 Starting metric updates...
✅ Metric logger updated in 0.439s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 33 (DataLoader iteration took 0.440s)...
Calculated step = 33, num_training_steps_per_epoch = 312
Global iteration it = 33
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 33...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.289s
🔧 Starting metric updates...
✅ Metric logger updated in 0.621s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 34 (DataLoader iteration took 0.623s)...
Calculated step = 34, num_training_steps_per_epoch = 312
Global iteration it = 34
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 34...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.229s
🔥 Total XLA step time: 0.277s
🔧 Starting metric updates...
✅ Metric logger updated in 0.621s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 35 (DataLoader iteration took 0.623s)...
Calculated step = 35, num_training_steps_per_epoch = 312
Global iteration it = 35
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 35...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.673s
🔧 Starting metric updates...
✅ Metric logger updated in 0.098s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 36 (DataLoader iteration took 0.100s)...
Calculated step = 36, num_training_steps_per_epoch = 312
Global iteration it = 36
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 36...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.628s
🔧 Starting metric updates...
✅ Metric logger updated in 0.214s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 37 (DataLoader iteration took 0.216s)...
Calculated step = 37, num_training_steps_per_epoch = 312
Global iteration it = 37
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 37...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.288s
🔧 Starting metric updates...
✅ Metric logger updated in 0.554s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 38 (DataLoader iteration took 0.555s)...
Calculated step = 38, num_training_steps_per_epoch = 312
Global iteration it = 38
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 38...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.187s
🔥 Total XLA step time: 0.627s
🔧 Starting metric updates...
✅ Metric logger updated in 0.147s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 39 (DataLoader iteration took 0.150s)...
Calculated step = 39, num_training_steps_per_epoch = 312
Global iteration it = 39
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 39...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.553s
🔧 Starting metric updates...
✅ Metric logger updated in 0.275s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 40 (DataLoader iteration took 0.276s)...
Calculated step = 40, num_training_steps_per_epoch = 312
Global iteration it = 40
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 40...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.284s
🔧 Starting metric updates...
✅ Metric logger updated in 0.504s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 40/312]  eta: 0:25:59  loss: 7.0312 (7.0412)  lr: 0.0000 (0.0000)  time: 0.8389  data: 0.0006
🚀 Starting data_iter_step 41 (DataLoader iteration took 0.507s)...
Calculated step = 41, num_training_steps_per_epoch = 312
Global iteration it = 41
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 41...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.288s
🔥 Total XLA step time: 0.674s
🔧 Starting metric updates...
✅ Metric logger updated in 0.081s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 42 (DataLoader iteration took 0.083s)...
Calculated step = 42, num_training_steps_per_epoch = 312
Global iteration it = 42
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 42...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.260s
🔥 Total XLA step time: 0.311s
🔧 Starting metric updates...
✅ Metric logger updated in 0.498s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 43 (DataLoader iteration took 0.499s)...
Calculated step = 43, num_training_steps_per_epoch = 312
Global iteration it = 43
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 43...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.15625, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.523s
🔧 Starting metric updates...
✅ Metric logger updated in 0.328s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 44 (DataLoader iteration took 0.330s)...
Calculated step = 44, num_training_steps_per_epoch = 312
Global iteration it = 44
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 44...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.284s
🔧 Starting metric updates...
✅ Metric logger updated in 0.476s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 45 (DataLoader iteration took 0.478s)...
Calculated step = 45, num_training_steps_per_epoch = 312
Global iteration it = 45
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 45...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.654s
🔧 Starting metric updates...
✅ Metric logger updated in 0.222s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 46 (DataLoader iteration took 0.224s)...
Calculated step = 46, num_training_steps_per_epoch = 312
Global iteration it = 46
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 46...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.285s
🔧 Starting metric updates...
✅ Metric logger updated in 0.517s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 47 (DataLoader iteration took 0.520s)...
Calculated step = 47, num_training_steps_per_epoch = 312
Global iteration it = 47
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 47...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.663s
🔧 Starting metric updates...
✅ Metric logger updated in 0.119s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 48 (DataLoader iteration took 0.120s)...
Calculated step = 48, num_training_steps_per_epoch = 312
Global iteration it = 48
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 48...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.286s
🔧 Starting metric updates...
✅ Metric logger updated in 0.542s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 49 (DataLoader iteration took 0.544s)...
Calculated step = 49, num_training_steps_per_epoch = 312
Global iteration it = 49
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 49...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.663s
🔧 Starting metric updates...
✅ Metric logger updated in 0.143s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 50 (DataLoader iteration took 0.145s)...
Calculated step = 50, num_training_steps_per_epoch = 312
Global iteration it = 50
TPU mode: Processing iteration 50, step 50, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 50...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.552s
TPU mode: Iteration 50, TPU memory: {'bytes_used': 1804986368, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 0.252s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 50/312]  eta: 0:20:49  loss: 7.0312 (7.0429)  lr: 0.0000 (0.0000)  time: 0.8215  data: 0.0006
🚀 Starting data_iter_step 51 (DataLoader iteration took 0.254s)...
Calculated step = 51, num_training_steps_per_epoch = 312
Global iteration it = 51
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 51...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.668s
🔧 Starting metric updates...
✅ Metric logger updated in 0.195s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 52 (DataLoader iteration took 0.196s)...
Calculated step = 52, num_training_steps_per_epoch = 312
Global iteration it = 52
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 52...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.293s
🔧 Starting metric updates...
✅ Metric logger updated in 0.575s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 53 (DataLoader iteration took 0.576s)...
Calculated step = 53, num_training_steps_per_epoch = 312
Global iteration it = 53
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 53...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.649s
🔧 Starting metric updates...
✅ Metric logger updated in 0.173s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 54 (DataLoader iteration took 0.174s)...
Calculated step = 54, num_training_steps_per_epoch = 312
Global iteration it = 54
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 54...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.291s
🔧 Starting metric updates...
✅ Metric logger updated in 0.449s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 55 (DataLoader iteration took 0.452s)...
Calculated step = 55, num_training_steps_per_epoch = 312
Global iteration it = 55
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 55...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.034s, about to compute loss...
✅ Loss computed in 0.004s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.319s
🔧 Starting metric updates...
✅ Metric logger updated in 0.413s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 56 (DataLoader iteration took 0.415s)...
Calculated step = 56, num_training_steps_per_epoch = 312
Global iteration it = 56
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 56...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.565s
🔧 Starting metric updates...
✅ Metric logger updated in 0.196s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 57 (DataLoader iteration took 0.198s)...
Calculated step = 57, num_training_steps_per_epoch = 312
Global iteration it = 57
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 57...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.292s
🔧 Starting metric updates...
✅ Metric logger updated in 0.536s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 58 (DataLoader iteration took 0.537s)...
Calculated step = 58, num_training_steps_per_epoch = 312
Global iteration it = 58
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 58...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.249s
🔥 Total XLA step time: 0.619s
🔧 Starting metric updates...
✅ Metric logger updated in 0.235s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 59 (DataLoader iteration took 0.236s)...
Calculated step = 59, num_training_steps_per_epoch = 312
Global iteration it = 59
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 59...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.287s
🔧 Starting metric updates...
✅ Metric logger updated in 0.597s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 60 (DataLoader iteration took 0.598s)...
Calculated step = 60, num_training_steps_per_epoch = 312
Global iteration it = 60
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 60...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.267s
🔥 Total XLA step time: 0.660s
🔧 Starting metric updates...
✅ Metric logger updated in 0.081s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 60/312]  eta: 0:17:18  loss: 7.0312 (7.0394)  lr: 0.0000 (0.0000)  time: 0.8132  data: 0.0006
🚀 Starting data_iter_step 61 (DataLoader iteration took 0.083s)...
Calculated step = 61, num_training_steps_per_epoch = 312
Global iteration it = 61
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 61...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.291s
🔧 Starting metric updates...
✅ Metric logger updated in 0.510s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 62 (DataLoader iteration took 0.511s)...
Calculated step = 62, num_training_steps_per_epoch = 312
Global iteration it = 62
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 62...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.020s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.257s
🔥 Total XLA step time: 0.312s
🔧 Starting metric updates...
✅ Metric logger updated in 0.521s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 63 (DataLoader iteration took 0.524s)...
Calculated step = 63, num_training_steps_per_epoch = 312
Global iteration it = 63
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 63...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.622s
🔧 Starting metric updates...
✅ Metric logger updated in 0.241s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 64 (DataLoader iteration took 0.242s)...
Calculated step = 64, num_training_steps_per_epoch = 312
Global iteration it = 64
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 64...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.858s
🔧 Starting metric updates...
✅ Metric logger updated in 0.081s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 65 (DataLoader iteration took 0.082s)...
Calculated step = 65, num_training_steps_per_epoch = 312
Global iteration it = 65
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 65...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.282s
🔧 Starting metric updates...
✅ Metric logger updated in 0.562s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 66 (DataLoader iteration took 0.563s)...
Calculated step = 66, num_training_steps_per_epoch = 312
Global iteration it = 66
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 66...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.651s
🔧 Starting metric updates...
✅ Metric logger updated in 0.246s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 67 (DataLoader iteration took 0.248s)...
Calculated step = 67, num_training_steps_per_epoch = 312
Global iteration it = 67
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 67...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.640s
🔧 Starting metric updates...
✅ Metric logger updated in 0.117s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 68 (DataLoader iteration took 0.119s)...
Calculated step = 68, num_training_steps_per_epoch = 312
Global iteration it = 68
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 68...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.658s
🔧 Starting metric updates...
✅ Metric logger updated in 0.432s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 69 (DataLoader iteration took 0.434s)...
Calculated step = 69, num_training_steps_per_epoch = 312
Global iteration it = 69
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 69...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.229s
🔥 Total XLA step time: 0.727s
🔧 Starting metric updates...
✅ Metric logger updated in 0.081s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 70 (DataLoader iteration took 0.083s)...
Calculated step = 70, num_training_steps_per_epoch = 312
Global iteration it = 70
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 70...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.194s
🔥 Total XLA step time: 0.501s
🔧 Starting metric updates...
✅ Metric logger updated in 0.294s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 70/312]  eta: 0:14:46  loss: 7.0312 (7.0379)  lr: 0.0000 (0.0000)  time: 0.8410  data: 0.0005
🚀 Starting data_iter_step 71 (DataLoader iteration took 0.301s)...
Calculated step = 71, num_training_steps_per_epoch = 312
Global iteration it = 71
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 71...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.660s
🔧 Starting metric updates...
✅ Metric logger updated in 0.183s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 72 (DataLoader iteration took 0.185s)...
Calculated step = 72, num_training_steps_per_epoch = 312
Global iteration it = 72
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 72...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.559s
🔧 Starting metric updates...
✅ Metric logger updated in 0.268s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 73 (DataLoader iteration took 0.269s)...
Calculated step = 73, num_training_steps_per_epoch = 312
Global iteration it = 73
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 73...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.591s
🔧 Starting metric updates...
✅ Metric logger updated in 0.298s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 74 (DataLoader iteration took 0.299s)...
Calculated step = 74, num_training_steps_per_epoch = 312
Global iteration it = 74
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 74...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.575s
🔧 Starting metric updates...
✅ Metric logger updated in 0.283s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 75 (DataLoader iteration took 0.285s)...
Calculated step = 75, num_training_steps_per_epoch = 312
Global iteration it = 75
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 75...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.291s
🔧 Starting metric updates...
✅ Metric logger updated in 0.521s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 76 (DataLoader iteration took 0.523s)...
Calculated step = 76, num_training_steps_per_epoch = 312
Global iteration it = 76
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 76...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.621s
🔧 Starting metric updates...
✅ Metric logger updated in 0.291s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 77 (DataLoader iteration took 0.293s)...
Calculated step = 77, num_training_steps_per_epoch = 312
Global iteration it = 77
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 77...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.286s
🔧 Starting metric updates...
✅ Metric logger updated in 0.557s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 78 (DataLoader iteration took 0.559s)...
Calculated step = 78, num_training_steps_per_epoch = 312
Global iteration it = 78
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 78...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.291s
🔧 Starting metric updates...
✅ Metric logger updated in 0.451s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 79 (DataLoader iteration took 0.455s)...
Calculated step = 79, num_training_steps_per_epoch = 312
Global iteration it = 79
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 79...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.018s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.295s
🔧 Starting metric updates...
✅ Metric logger updated in 0.497s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 80 (DataLoader iteration took 0.499s)...
Calculated step = 80, num_training_steps_per_epoch = 312
Global iteration it = 80
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 80...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.733s
🔧 Starting metric updates...
✅ Metric logger updated in 0.110s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 80/312]  eta: 0:12:48  loss: 7.0312 (7.0351)  lr: 0.0000 (0.0000)  time: 0.8556  data: 0.0006
🚀 Starting data_iter_step 81 (DataLoader iteration took 0.113s)...
Calculated step = 81, num_training_steps_per_epoch = 312
Global iteration it = 81
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 81...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.293s
🔧 Starting metric updates...
✅ Metric logger updated in 0.502s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 82 (DataLoader iteration took 0.503s)...
Calculated step = 82, num_training_steps_per_epoch = 312
Global iteration it = 82
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 82...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.777s
🔧 Starting metric updates...
✅ Metric logger updated in 0.090s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 83 (DataLoader iteration took 0.092s)...
Calculated step = 83, num_training_steps_per_epoch = 312
Global iteration it = 83
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 83...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.288s
🔧 Starting metric updates...
✅ Metric logger updated in 0.620s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 84 (DataLoader iteration took 0.621s)...
Calculated step = 84, num_training_steps_per_epoch = 312
Global iteration it = 84
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 84...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.656s
🔧 Starting metric updates...
✅ Metric logger updated in 0.440s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 85 (DataLoader iteration took 0.442s)...
Calculated step = 85, num_training_steps_per_epoch = 312
Global iteration it = 85
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 85...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.594s
🔧 Starting metric updates...
✅ Metric logger updated in 0.268s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 86 (DataLoader iteration took 0.269s)...
Calculated step = 86, num_training_steps_per_epoch = 312
Global iteration it = 86
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 86...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.206s
🔥 Total XLA step time: 0.257s
🔧 Starting metric updates...
✅ Metric logger updated in 0.565s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 87 (DataLoader iteration took 0.571s)...
Calculated step = 87, num_training_steps_per_epoch = 312
Global iteration it = 87
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 87...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.023s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.309s
🔧 Starting metric updates...
✅ Metric logger updated in 0.536s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 88 (DataLoader iteration took 0.538s)...
Calculated step = 88, num_training_steps_per_epoch = 312
Global iteration it = 88
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 88...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.627s
🔧 Starting metric updates...
✅ Metric logger updated in 0.127s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 89 (DataLoader iteration took 0.129s)...
Calculated step = 89, num_training_steps_per_epoch = 312
Global iteration it = 89
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 89...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.266s
🔥 Total XLA step time: 0.757s
🔧 Starting metric updates...
✅ Metric logger updated in 0.105s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 90 (DataLoader iteration took 0.107s)...
Calculated step = 90, num_training_steps_per_epoch = 312
Global iteration it = 90
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 90...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.289s
🔧 Starting metric updates...
✅ Metric logger updated in 0.505s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 90/312]  eta: 0:11:16  loss: 7.0312 (7.0343)  lr: 0.0001 (0.0000)  time: 0.8545  data: 0.0007
🚀 Starting data_iter_step 91 (DataLoader iteration took 0.507s)...
Calculated step = 91, num_training_steps_per_epoch = 312
Global iteration it = 91
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 91...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.266s
🔥 Total XLA step time: 0.318s
🔧 Starting metric updates...
✅ Metric logger updated in 0.497s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 92 (DataLoader iteration took 0.499s)...
Calculated step = 92, num_training_steps_per_epoch = 312
Global iteration it = 92
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 92...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.020s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.260s
🔥 Total XLA step time: 0.316s
🔧 Starting metric updates...
✅ Metric logger updated in 0.474s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 93 (DataLoader iteration took 0.476s)...
Calculated step = 93, num_training_steps_per_epoch = 312
Global iteration it = 93
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 93...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.628s
🔧 Starting metric updates...
✅ Metric logger updated in 0.231s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 94 (DataLoader iteration took 0.233s)...
Calculated step = 94, num_training_steps_per_epoch = 312
Global iteration it = 94
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 94...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.621s
🔧 Starting metric updates...
✅ Metric logger updated in 0.155s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 95 (DataLoader iteration took 0.159s)...
Calculated step = 95, num_training_steps_per_epoch = 312
Global iteration it = 95
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 95...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.649s
🔧 Starting metric updates...
✅ Metric logger updated in 0.248s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 96 (DataLoader iteration took 0.249s)...
Calculated step = 96, num_training_steps_per_epoch = 312
Global iteration it = 96
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 96...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.288s
🔧 Starting metric updates...
✅ Metric logger updated in 0.593s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 97 (DataLoader iteration took 0.595s)...
Calculated step = 97, num_training_steps_per_epoch = 312
Global iteration it = 97
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 97...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.630s
🔧 Starting metric updates...
✅ Metric logger updated in 0.140s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 98 (DataLoader iteration took 0.142s)...
Calculated step = 98, num_training_steps_per_epoch = 312
Global iteration it = 98
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 98...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.295s
🔧 Starting metric updates...
✅ Metric logger updated in 0.570s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 99 (DataLoader iteration took 0.572s)...
Calculated step = 99, num_training_steps_per_epoch = 312
Global iteration it = 99
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 99...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.628s
🔧 Starting metric updates...
✅ Metric logger updated in 0.124s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 100 (DataLoader iteration took 0.125s)...
Calculated step = 100, num_training_steps_per_epoch = 312
Global iteration it = 100
TPU mode: Processing iteration 100, step 100, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 100...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.628s
TPU mode: Iteration 100, TPU memory: {'bytes_used': 1737391616, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 0.110s
✅ Wandb logged in 0.000s
Epoch: [0]  [100/312]  eta: 0:09:58  loss: 7.0000 (7.0306)  lr: 0.0001 (0.0000)  time: 0.8428  data: 0.0006
🚀 Starting data_iter_step 101 (DataLoader iteration took 0.113s)...
Calculated step = 101, num_training_steps_per_epoch = 312
Global iteration it = 101
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 101...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.612s
🔧 Starting metric updates...
✅ Metric logger updated in 0.228s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 102 (DataLoader iteration took 0.229s)...
Calculated step = 102, num_training_steps_per_epoch = 312
Global iteration it = 102
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 102...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.293s
🔧 Starting metric updates...
✅ Metric logger updated in 0.526s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 103 (DataLoader iteration took 0.530s)...
Calculated step = 103, num_training_steps_per_epoch = 312
Global iteration it = 103
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 103...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.027s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.313s
🔧 Starting metric updates...
✅ Metric logger updated in 0.547s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 104 (DataLoader iteration took 0.548s)...
Calculated step = 104, num_training_steps_per_epoch = 312
Global iteration it = 104
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 104...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.602s
🔧 Starting metric updates...
✅ Metric logger updated in 0.256s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 105 (DataLoader iteration took 0.257s)...
Calculated step = 105, num_training_steps_per_epoch = 312
Global iteration it = 105
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 105...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.639s
🔧 Starting metric updates...
✅ Metric logger updated in 0.216s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 106 (DataLoader iteration took 0.218s)...
Calculated step = 106, num_training_steps_per_epoch = 312
Global iteration it = 106
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 106...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.627s
🔧 Starting metric updates...
✅ Metric logger updated in 0.193s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 107 (DataLoader iteration took 0.194s)...
Calculated step = 107, num_training_steps_per_epoch = 312
Global iteration it = 107
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 107...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.738s
🔧 Starting metric updates...
✅ Metric logger updated in 0.101s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 108 (DataLoader iteration took 0.103s)...
Calculated step = 108, num_training_steps_per_epoch = 312
Global iteration it = 108
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 108...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.296s
🔧 Starting metric updates...
✅ Metric logger updated in 0.502s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 109 (DataLoader iteration took 0.504s)...
Calculated step = 109, num_training_steps_per_epoch = 312
Global iteration it = 109
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 109...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.286s
🔧 Starting metric updates...
✅ Metric logger updated in 0.551s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 110 (DataLoader iteration took 0.553s)...
Calculated step = 110, num_training_steps_per_epoch = 312
Global iteration it = 110
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 110...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.296s
🔧 Starting metric updates...
✅ Metric logger updated in 0.441s
✅ Wandb logged in 0.000s
Epoch: [0]  [110/312]  eta: 0:08:54  loss: 7.0000 (7.0273)  lr: 0.0001 (0.0000)  time: 0.8254  data: 0.0006
🚀 Starting data_iter_step 111 (DataLoader iteration took 0.446s)...
Calculated step = 111, num_training_steps_per_epoch = 312
Global iteration it = 111
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 111...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.028s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.310s
🔧 Starting metric updates...
✅ Metric logger updated in 0.448s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 112 (DataLoader iteration took 0.450s)...
Calculated step = 112, num_training_steps_per_epoch = 312
Global iteration it = 112
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 112...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.289s
🔧 Starting metric updates...
✅ Metric logger updated in 0.557s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 113 (DataLoader iteration took 0.559s)...
Calculated step = 113, num_training_steps_per_epoch = 312
Global iteration it = 113
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 113...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.605s
🔧 Starting metric updates...
✅ Metric logger updated in 0.223s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 114 (DataLoader iteration took 0.225s)...
Calculated step = 114, num_training_steps_per_epoch = 312
Global iteration it = 114
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 114...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.296s
🔧 Starting metric updates...
✅ Metric logger updated in 0.589s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 115 (DataLoader iteration took 0.591s)...
Calculated step = 115, num_training_steps_per_epoch = 312
Global iteration it = 115
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 115...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.634s
🔧 Starting metric updates...
✅ Metric logger updated in 0.216s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 116 (DataLoader iteration took 0.218s)...
Calculated step = 116, num_training_steps_per_epoch = 312
Global iteration it = 116
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 116...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.288s
🔧 Starting metric updates...
✅ Metric logger updated in 0.599s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 117 (DataLoader iteration took 0.601s)...
Calculated step = 117, num_training_steps_per_epoch = 312
Global iteration it = 117
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 117...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.227s
🔥 Total XLA step time: 0.277s
🔧 Starting metric updates...
✅ Metric logger updated in 0.516s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 118 (DataLoader iteration took 0.518s)...
Calculated step = 118, num_training_steps_per_epoch = 312
Global iteration it = 118
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 118...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.203s
🔥 Total XLA step time: 0.584s
🔧 Starting metric updates...
✅ Metric logger updated in 0.168s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 119 (DataLoader iteration took 0.171s)...
Calculated step = 119, num_training_steps_per_epoch = 312
Global iteration it = 119
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 119...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.019s, about to compute loss...
✅ Loss computed in 0.002s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.310s
🔧 Starting metric updates...
✅ Metric logger updated in 0.538s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 120 (DataLoader iteration took 0.540s)...
Calculated step = 120, num_training_steps_per_epoch = 312
Global iteration it = 120
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 120...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.642s
🔧 Starting metric updates...
✅ Metric logger updated in 0.212s
✅ Wandb logged in 0.000s
Epoch: [0]  [120/312]  eta: 0:07:59  loss: 6.9688 (7.0240)  lr: 0.0001 (0.0000)  time: 0.8338  data: 0.0007
🚀 Starting data_iter_step 121 (DataLoader iteration took 0.215s)...
Calculated step = 121, num_training_steps_per_epoch = 312
Global iteration it = 121
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 121...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.633s
🔧 Starting metric updates...
✅ Metric logger updated in 0.154s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 122 (DataLoader iteration took 0.156s)...
Calculated step = 122, num_training_steps_per_epoch = 312
Global iteration it = 122
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 122...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.018s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.293s
🔧 Starting metric updates...
✅ Metric logger updated in 0.487s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 123 (DataLoader iteration took 0.490s)...
Calculated step = 123, num_training_steps_per_epoch = 312
Global iteration it = 123
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 123...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.621s
🔧 Starting metric updates...
✅ Metric logger updated in 0.219s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 124 (DataLoader iteration took 0.221s)...
Calculated step = 124, num_training_steps_per_epoch = 312
Global iteration it = 124
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 124...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.614s
🔧 Starting metric updates...
✅ Metric logger updated in 0.265s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 125 (DataLoader iteration took 0.266s)...
Calculated step = 125, num_training_steps_per_epoch = 312
Global iteration it = 125
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 125...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.020s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.627s
🔧 Starting metric updates...
✅ Metric logger updated in 0.173s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 126 (DataLoader iteration took 0.174s)...
Calculated step = 126, num_training_steps_per_epoch = 312
Global iteration it = 126
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 126...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.197s
🔥 Total XLA step time: 0.601s
🔧 Starting metric updates...
✅ Metric logger updated in 0.182s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 127 (DataLoader iteration took 0.186s)...
Calculated step = 127, num_training_steps_per_epoch = 312
Global iteration it = 127
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 127...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.021s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.313s
🔧 Starting metric updates...
✅ Metric logger updated in 0.523s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 128 (DataLoader iteration took 0.524s)...
Calculated step = 128, num_training_steps_per_epoch = 312
Global iteration it = 128
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 128...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.288s
🔧 Starting metric updates...
✅ Metric logger updated in 0.691s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 129 (DataLoader iteration took 0.693s)...
Calculated step = 129, num_training_steps_per_epoch = 312
Global iteration it = 129
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 129...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.292s
🔧 Starting metric updates...
✅ Metric logger updated in 0.549s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 130 (DataLoader iteration took 0.551s)...
Calculated step = 130, num_training_steps_per_epoch = 312
Global iteration it = 130
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 130...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.285s
🔧 Starting metric updates...
✅ Metric logger updated in 0.460s
✅ Wandb logged in 0.000s
Epoch: [0]  [130/312]  eta: 0:07:11  loss: 7.0000 (7.0222)  lr: 0.0001 (0.0000)  time: 0.8343  data: 0.0007
🚀 Starting data_iter_step 131 (DataLoader iteration took 0.463s)...
Calculated step = 131, num_training_steps_per_epoch = 312
Global iteration it = 131
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 131...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.294s
🔧 Starting metric updates...
✅ Metric logger updated in 0.555s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 132 (DataLoader iteration took 0.558s)...
Calculated step = 132, num_training_steps_per_epoch = 312
Global iteration it = 132
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 132...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.290s
🔧 Starting metric updates...
✅ Metric logger updated in 0.550s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 133 (DataLoader iteration took 0.552s)...
Calculated step = 133, num_training_steps_per_epoch = 312
Global iteration it = 133
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 133...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.017s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.295s
🔧 Starting metric updates...
✅ Metric logger updated in 0.494s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 134 (DataLoader iteration took 0.496s)...
Calculated step = 134, num_training_steps_per_epoch = 312
Global iteration it = 134
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 134...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.294s
🔧 Starting metric updates...
✅ Metric logger updated in 0.452s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 135 (DataLoader iteration took 0.457s)...
Calculated step = 135, num_training_steps_per_epoch = 312
Global iteration it = 135
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 135...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.021s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.231s
🔥 Total XLA step time: 0.295s
🔧 Starting metric updates...
✅ Metric logger updated in 0.850s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 136 (DataLoader iteration took 0.852s)...
Calculated step = 136, num_training_steps_per_epoch = 312
Global iteration it = 136
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 136...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.629s
🔧 Starting metric updates...
✅ Metric logger updated in 0.208s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 137 (DataLoader iteration took 0.210s)...
Calculated step = 137, num_training_steps_per_epoch = 312
Global iteration it = 137
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 137...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.228s
🔥 Total XLA step time: 0.277s
🔧 Starting metric updates...
✅ Metric logger updated in 0.496s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 138 (DataLoader iteration took 0.498s)...
Calculated step = 138, num_training_steps_per_epoch = 312
Global iteration it = 138
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 138...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.225s
🔥 Total XLA step time: 0.541s
🔧 Starting metric updates...
✅ Metric logger updated in 0.303s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 139 (DataLoader iteration took 0.305s)...
Calculated step = 139, num_training_steps_per_epoch = 312
Global iteration it = 139
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 139...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.283s
🔧 Starting metric updates...
✅ Metric logger updated in 0.512s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 140 (DataLoader iteration took 0.514s)...
Calculated step = 140, num_training_steps_per_epoch = 312
Global iteration it = 140
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 140...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.282s
🔧 Starting metric updates...
✅ Metric logger updated in 0.640s
✅ Wandb logged in 0.000s
Epoch: [0]  [140/312]  eta: 0:06:29  loss: 7.0000 (7.0180)  lr: 0.0001 (0.0000)  time: 0.8459  data: 0.0006
🚀 Starting data_iter_step 141 (DataLoader iteration took 0.642s)...
Calculated step = 141, num_training_steps_per_epoch = 312
Global iteration it = 141
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 141...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.224s
🔥 Total XLA step time: 0.274s
🔧 Starting metric updates...
✅ Metric logger updated in 0.518s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 142 (DataLoader iteration took 0.520s)...
Calculated step = 142, num_training_steps_per_epoch = 312
Global iteration it = 142
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 142...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.185s
🔥 Total XLA step time: 0.561s
🔧 Starting metric updates...
✅ Metric logger updated in 0.204s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 143 (DataLoader iteration took 0.207s)...
Calculated step = 143, num_training_steps_per_epoch = 312
Global iteration it = 143
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 143...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.751s
🔧 Starting metric updates...
✅ Metric logger updated in 0.082s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 144 (DataLoader iteration took 0.083s)...
Calculated step = 144, num_training_steps_per_epoch = 312
Global iteration it = 144
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 144...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.288s
🔧 Starting metric updates...
✅ Metric logger updated in 0.526s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 145 (DataLoader iteration took 0.528s)...
Calculated step = 145, num_training_steps_per_epoch = 312
Global iteration it = 145
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 145...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.285s
🔧 Starting metric updates...
✅ Metric logger updated in 0.587s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 146 (DataLoader iteration took 0.588s)...
Calculated step = 146, num_training_steps_per_epoch = 312
Global iteration it = 146
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 146...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.598s
🔧 Starting metric updates...
✅ Metric logger updated in 0.260s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 147 (DataLoader iteration took 0.261s)...
Calculated step = 147, num_training_steps_per_epoch = 312
Global iteration it = 147
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 147...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.231s
🔥 Total XLA step time: 0.282s
🔧 Starting metric updates...
✅ Metric logger updated in 0.516s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 148 (DataLoader iteration took 0.518s)...
Calculated step = 148, num_training_steps_per_epoch = 312
Global iteration it = 148
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 148...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.227s
🔥 Total XLA step time: 0.614s
🔧 Starting metric updates...
✅ Metric logger updated in 0.212s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 149 (DataLoader iteration took 0.214s)...
Calculated step = 149, num_training_steps_per_epoch = 312
Global iteration it = 149
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 149...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.653s
🔧 Starting metric updates...
✅ Metric logger updated in 0.192s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 150 (DataLoader iteration took 0.193s)...
Calculated step = 150, num_training_steps_per_epoch = 312
Global iteration it = 150
TPU mode: Processing iteration 150, step 150, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 150...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.228s
🔥 Total XLA step time: 0.280s
TPU mode: Iteration 150, TPU memory: {'bytes_used': 1666655232, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 0.511s
✅ Wandb logged in 0.000s
Epoch: [0]  [150/312]  eta: 0:05:50  loss: 6.9688 (7.0145)  lr: 0.0001 (0.0000)  time: 0.8419  data: 0.0006
🚀 Starting data_iter_step 151 (DataLoader iteration took 0.516s)...
Calculated step = 151, num_training_steps_per_epoch = 312
Global iteration it = 151
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 151...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.019s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.305s
🔧 Starting metric updates...
✅ Metric logger updated in 0.507s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 152 (DataLoader iteration took 0.509s)...
Calculated step = 152, num_training_steps_per_epoch = 312
Global iteration it = 152
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 152...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.534s
🔧 Starting metric updates...
✅ Metric logger updated in 0.369s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 153 (DataLoader iteration took 0.370s)...
Calculated step = 153, num_training_steps_per_epoch = 312
Global iteration it = 153
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 153...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.297s
🔧 Starting metric updates...
✅ Metric logger updated in 0.557s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 154 (DataLoader iteration took 0.558s)...
Calculated step = 154, num_training_steps_per_epoch = 312
Global iteration it = 154
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 154...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.291s
🔧 Starting metric updates...
✅ Metric logger updated in 0.538s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 155 (DataLoader iteration took 0.539s)...
Calculated step = 155, num_training_steps_per_epoch = 312
Global iteration it = 155
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 155...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.293s
🔧 Starting metric updates...
✅ Metric logger updated in 0.520s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 156 (DataLoader iteration took 0.522s)...
Calculated step = 156, num_training_steps_per_epoch = 312
Global iteration it = 156
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 156...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.286s
🔧 Starting metric updates...
✅ Metric logger updated in 0.487s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 157 (DataLoader iteration took 0.489s)...
Calculated step = 157, num_training_steps_per_epoch = 312
Global iteration it = 157
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 157...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.291s
🔧 Starting metric updates...
✅ Metric logger updated in 0.464s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 158 (DataLoader iteration took 0.465s)...
Calculated step = 158, num_training_steps_per_epoch = 312
Global iteration it = 158
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 158...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.293s
🔧 Starting metric updates...
✅ Metric logger updated in 0.564s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 159 (DataLoader iteration took 0.569s)...
Calculated step = 159, num_training_steps_per_epoch = 312
Global iteration it = 159
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 159...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.023s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.303s
🔧 Starting metric updates...
✅ Metric logger updated in 0.785s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 160 (DataLoader iteration took 0.787s)...
Calculated step = 160, num_training_steps_per_epoch = 312
Global iteration it = 160
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 160...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.560s
🔧 Starting metric updates...
✅ Metric logger updated in 0.238s
✅ Wandb logged in 0.000s
Epoch: [0]  [160/312]  eta: 0:05:16  loss: 6.9688 (7.0116)  lr: 0.0001 (0.0001)  time: 0.8396  data: 0.0007
🚀 Starting data_iter_step 161 (DataLoader iteration took 0.241s)...
Calculated step = 161, num_training_steps_per_epoch = 312
Global iteration it = 161
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 161...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.295s
🔧 Starting metric updates...
✅ Metric logger updated in 0.607s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 162 (DataLoader iteration took 0.609s)...
Calculated step = 162, num_training_steps_per_epoch = 312
Global iteration it = 162
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 162...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.291s
🔧 Starting metric updates...
✅ Metric logger updated in 0.490s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 163 (DataLoader iteration took 0.491s)...
Calculated step = 163, num_training_steps_per_epoch = 312
Global iteration it = 163
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 163...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.292s
🔧 Starting metric updates...
✅ Metric logger updated in 0.544s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 164 (DataLoader iteration took 0.546s)...
Calculated step = 164, num_training_steps_per_epoch = 312
Global iteration it = 164
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 164...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.284s
🔧 Starting metric updates...
✅ Metric logger updated in 0.601s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 165 (DataLoader iteration took 0.602s)...
Calculated step = 165, num_training_steps_per_epoch = 312
Global iteration it = 165
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 165...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.613s
🔧 Starting metric updates...
✅ Metric logger updated in 0.155s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 166 (DataLoader iteration took 0.156s)...
Calculated step = 166, num_training_steps_per_epoch = 312
Global iteration it = 166
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 166...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.198s
🔥 Total XLA step time: 0.249s
🔧 Starting metric updates...
✅ Metric logger updated in 0.452s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 167 (DataLoader iteration took 0.456s)...
Calculated step = 167, num_training_steps_per_epoch = 312
Global iteration it = 167
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 167...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.647s
🔧 Starting metric updates...
✅ Metric logger updated in 0.104s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 168 (DataLoader iteration took 0.107s)...
Calculated step = 168, num_training_steps_per_epoch = 312
Global iteration it = 168
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 168...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.249s
🔥 Total XLA step time: 0.297s
🔧 Starting metric updates...
✅ Metric logger updated in 0.555s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 169 (DataLoader iteration took 0.557s)...
Calculated step = 169, num_training_steps_per_epoch = 312
Global iteration it = 169
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 169...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.605s
🔧 Starting metric updates...
✅ Metric logger updated in 0.167s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 170 (DataLoader iteration took 0.169s)...
Calculated step = 170, num_training_steps_per_epoch = 312
Global iteration it = 170
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 170...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.286s
🔧 Starting metric updates...
✅ Metric logger updated in 0.569s
✅ Wandb logged in 0.001s
Epoch: [0]  [170/312]  eta: 0:04:45  loss: 6.9688 (7.0077)  lr: 0.0001 (0.0001)  time: 0.8353  data: 0.0006
🚀 Starting data_iter_step 171 (DataLoader iteration took 0.571s)...
Calculated step = 171, num_training_steps_per_epoch = 312
Global iteration it = 171
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 171...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.288s
🔧 Starting metric updates...
✅ Metric logger updated in 0.508s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 172 (DataLoader iteration took 0.510s)...
Calculated step = 172, num_training_steps_per_epoch = 312
Global iteration it = 172
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 172...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.680s
🔧 Starting metric updates...
✅ Metric logger updated in 0.225s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 173 (DataLoader iteration took 0.228s)...
Calculated step = 173, num_training_steps_per_epoch = 312
Global iteration it = 173
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 173...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.580s
🔧 Starting metric updates...
✅ Metric logger updated in 0.183s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 174 (DataLoader iteration took 0.184s)...
Calculated step = 174, num_training_steps_per_epoch = 312
Global iteration it = 174
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 174...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.282s
🔧 Starting metric updates...
✅ Metric logger updated in 0.518s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 175 (DataLoader iteration took 0.521s)...
Calculated step = 175, num_training_steps_per_epoch = 312
Global iteration it = 175
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 175...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.021s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.307s
🔧 Starting metric updates...
✅ Metric logger updated in 0.554s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 176 (DataLoader iteration took 0.556s)...
Calculated step = 176, num_training_steps_per_epoch = 312
Global iteration it = 176
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 176...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.251s
🔥 Total XLA step time: 0.301s
🔧 Starting metric updates...
✅ Metric logger updated in 0.587s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 177 (DataLoader iteration took 0.588s)...
Calculated step = 177, num_training_steps_per_epoch = 312
Global iteration it = 177
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 177...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.017s
About to run optimizer step...
✅ Optimizer step completed in 0.229s
🔥 Total XLA step time: 0.282s
🔧 Starting metric updates...
✅ Metric logger updated in 0.792s
✅ Wandb logged in 0.001s
🚀 Starting data_iter_step 178 (DataLoader iteration took 0.794s)...
Calculated step = 178, num_training_steps_per_epoch = 312
Global iteration it = 178
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 178...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.295s
