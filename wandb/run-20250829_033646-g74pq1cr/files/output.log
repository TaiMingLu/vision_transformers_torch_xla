W&B online run started. URL: https://wandb.ai/ttl/tpu-vit/runs/g74pq1cr
Mixup is activated!
WARNING:timm.models._builder:No pretrained configuration specified for my_vit_b model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Forcing XLA synchronization after model.to(device)...
XLA mark_step() after model.to(device) completed
Forcing XLA synchronization before parameter counting...
XLA mark_step() completed
TPU spawned process: world_size = 32
Calculated total_batch_size = 4096
Number of training examples = 1281167
LR = 0.00400000
Batch size = 4096
Update frequent = 1
Number of training steps per epoch = 312
TPU multihost mode: XLA handles data distribution internally
About to create optimizer...
TPU mode: Using name-only parameter grouping to avoid XLA compilation
TPU mode: Collecting parameter names...
TPU mode: Found 152 trainable parameters
TPU mode: Grouping parameters by names...
Processing parameter 1/152: cls_token
Processing parameter 51/152: blocks.3.mlp.fc2.weight
Processing parameter 101/152: blocks.8.norm1.weight
Processing parameter 151/152: head.weight
TPU mode: Parameter names grouped successfully
TPU mode: Mapping names to tensors...
TPU mode: All 152 parameters mapped successfully
Parameter iteration completed! Processed 152 parameters.
TPU mode: Created 2 parameter groups, returning optimizer groups...
Parameter groups created successfully
Optimizer created successfully
TPU mode: Skipping loss scaler (XLA handles mixed precision)
TPU mode: About to set loss_scaler = None...
TPU mode: Loss scaler set to None
About to create Cosine LR scheduler...
Set warmup steps = 6240
LR scheduler created successfully
About to continue to weight decay scheduler...
About to check args.weight_decay_end...
Directly setting weight_decay_end to avoid freeze...
weight_decay_end set to: 0.05
About to call cosine_scheduler for weight decay...
Calling with: wd=0.05, wd_end=0.05, epochs=100, steps_per_epoch=312
Set warmup steps = 0
cosine_scheduler completed, about to calculate min/max...
Weight decay schedule created: 0.05 -> 0.05
About to create criterion...
Created SoftTargetCrossEntropy criterion
criterion = SoftTargetCrossEntropy()
TPU mode: Skipping warmup (caused shape errors), will compile on first iteration
About to call auto_load_model...
auto_load_model completed
About to define get_eval_model helper function...
get_eval_model function defined
About to set max_accuracy...
max_accuracy initialized
Start training for 100 epochs
About to record start_time...
About to enter training loop...
Starting epoch 0...
Setting sampler epoch...
Setting wandb steps...
About to call train_one_epoch...
train_one_epoch started: epoch=0, tpu=True
About to zero_grad optimizer...
About to start data loader loop...
Starting data_iter_step 0...
Calculated step = 0, num_training_steps_per_epoch = 312
Global iteration it = 0
TPU mode: Processing iteration 0, step 0, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 0...
TPU mode: Current device = xla:0, device type = xla
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.001s
TPU mode: samples.device = xla:0
TPU mode: targets.device = xla:0
TPU mode: model device = xla:0
About to call model forward...
âœ… Model forward completed in 0.020s, about to compute loss...
âœ… Loss computed in 0.000s: 7.125, about to backward...
âœ… Backward completed in 0.023s
About to run optimizer step...
âœ… Optimizer step completed in 26.823s
ðŸ”¥ Total XLA step time: 39.074s
TPU mode: Iteration 0 completed in 39.08s
TPU mode: Iteration 0, TPU memory: {'bytes_used': 1119007744, 'bytes_limit': 33550237696}
Epoch: [0]  [  0/312]  eta: 5:00:45  loss: 7.1250 (7.1250)  lr: 0.0000 (0.0000)  time: 57.8366  data: 0.7207
Starting data_iter_step 1...
Calculated step = 1, num_training_steps_per_epoch = 312
Global iteration it = 1
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 1...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.019s
About to call model forward...
âœ… Model forward completed in 0.012s, about to compute loss...
âœ… Loss computed in 0.000s: 7.0625, about to backward...
âœ… Backward completed in 0.015s
About to run optimizer step...
âœ… Optimizer step completed in 28.449s
ðŸ”¥ Total XLA step time: 28.586s
TPU mode: Iteration 1 completed in 28.65s
Starting data_iter_step 2...
Calculated step = 2, num_training_steps_per_epoch = 312
Global iteration it = 2
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 2...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.003s
About to call model forward...
âœ… Model forward completed in 0.014s, about to compute loss...
âœ… Loss computed in 0.001s: 7.15625, about to backward...
âœ… Backward completed in 0.012s
About to run optimizer step...
âœ… Optimizer step completed in 0.184s
ðŸ”¥ Total XLA step time: 0.238s
TPU mode: Iteration 2 completed in 0.24s
Starting data_iter_step 3...
Calculated step = 3, num_training_steps_per_epoch = 312
Global iteration it = 3
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 3...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.018s
About to call model forward...
âœ… Model forward completed in 0.013s, about to compute loss...
âœ… Loss computed in 0.001s: 7.125, about to backward...
âœ… Backward completed in 0.012s
About to run optimizer step...
âœ… Optimizer step completed in 0.185s
ðŸ”¥ Total XLA step time: 0.253s
TPU mode: Iteration 3 completed in 0.36s
Starting data_iter_step 4...
Calculated step = 4, num_training_steps_per_epoch = 312
Global iteration it = 4
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 4...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.001s
About to call model forward...
âœ… Model forward completed in 0.012s, about to compute loss...
âœ… Loss computed in 0.001s: 7.0, about to backward...
âœ… Backward completed in 0.012s
About to run optimizer step...
âœ… Optimizer step completed in 0.185s
ðŸ”¥ Total XLA step time: 0.233s
TPU mode: Iteration 4 completed in 0.30s
Starting data_iter_step 5...
Calculated step = 5, num_training_steps_per_epoch = 312
Global iteration it = 5
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 5...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.006s
About to call model forward...
âœ… Model forward completed in 0.015s, about to compute loss...
âœ… Loss computed in 0.001s: 7.0, about to backward...
âœ… Backward completed in 0.013s
About to run optimizer step...
âœ… Optimizer step completed in 0.186s
ðŸ”¥ Total XLA step time: 0.243s
Starting data_iter_step 6...
Calculated step = 6, num_training_steps_per_epoch = 312
Global iteration it = 6
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 6...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.032s
About to call model forward...
âœ… Model forward completed in 0.020s, about to compute loss...
âœ… Loss computed in 0.001s: 7.0625, about to backward...
âœ… Backward completed in 0.020s
About to run optimizer step...
âœ… Optimizer step completed in 0.211s
ðŸ”¥ Total XLA step time: 0.311s
Starting data_iter_step 7...
Calculated step = 7, num_training_steps_per_epoch = 312
Global iteration it = 7
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 7...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.029s
About to call model forward...
âœ… Model forward completed in 0.017s, about to compute loss...
âœ… Loss computed in 0.000s: 7.09375, about to backward...
âœ… Backward completed in 0.023s
About to run optimizer step...
âœ… Optimizer step completed in 0.199s
ðŸ”¥ Total XLA step time: 0.295s
Starting data_iter_step 8...
Calculated step = 8, num_training_steps_per_epoch = 312
Global iteration it = 8
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 8...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.036s
About to call model forward...
âœ… Model forward completed in 0.013s, about to compute loss...
âœ… Loss computed in 0.001s: 7.09375, about to backward...
âœ… Backward completed in 0.013s
About to run optimizer step...
âœ… Optimizer step completed in 0.197s
ðŸ”¥ Total XLA step time: 0.287s
Starting data_iter_step 9...
Calculated step = 9, num_training_steps_per_epoch = 312
Global iteration it = 9
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 9...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.014s
About to call model forward...
âœ… Model forward completed in 0.012s, about to compute loss...
âœ… Loss computed in 0.000s: 7.0625, about to backward...
âœ… Backward completed in 0.016s
About to run optimizer step...
âœ… Optimizer step completed in 0.185s
ðŸ”¥ Total XLA step time: 0.259s
Starting data_iter_step 10...
Calculated step = 10, num_training_steps_per_epoch = 312
Global iteration it = 10
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 10...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.017s
About to call model forward...
âœ… Model forward completed in 0.018s, about to compute loss...
âœ… Loss computed in 0.001s: 7.09375, about to backward...
âœ… Backward completed in 0.012s
About to run optimizer step...
âœ… Optimizer step completed in 0.186s
ðŸ”¥ Total XLA step time: 0.264s
Epoch: [0]  [ 10/312]  eta: 1:54:02  loss: 7.0938 (7.0795)  lr: 0.0000 (0.0000)  time: 22.6557  data: 14.1355
Starting data_iter_step 11...
Calculated step = 11, num_training_steps_per_epoch = 312
Global iteration it = 11
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 11...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.004s
About to call model forward...
âœ… Model forward completed in 0.013s, about to compute loss...
âœ… Loss computed in 0.001s: 7.0, about to backward...
âœ… Backward completed in 0.012s
About to run optimizer step...
âœ… Optimizer step completed in 0.185s
ðŸ”¥ Total XLA step time: 0.238s
Starting data_iter_step 12...
Calculated step = 12, num_training_steps_per_epoch = 312
Global iteration it = 12
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 12...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.012s
About to call model forward...
âœ… Model forward completed in 0.016s, about to compute loss...
âœ… Loss computed in 0.001s: 7.0625, about to backward...
âœ… Backward completed in 0.014s
About to run optimizer step...
âœ… Optimizer step completed in 0.186s
ðŸ”¥ Total XLA step time: 0.258s
Starting data_iter_step 13...
Calculated step = 13, num_training_steps_per_epoch = 312
Global iteration it = 13
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 13...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.036s
About to call model forward...
âœ… Model forward completed in 0.014s, about to compute loss...
âœ… Loss computed in 0.000s: 7.0625, about to backward...
âœ… Backward completed in 0.033s
About to run optimizer step...
âœ… Optimizer step completed in 0.240s
ðŸ”¥ Total XLA step time: 0.356s
Starting data_iter_step 14...
Calculated step = 14, num_training_steps_per_epoch = 312
Global iteration it = 14
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 14...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.024s
About to call model forward...
âœ… Model forward completed in 0.012s, about to compute loss...
âœ… Loss computed in 0.000s: 7.125, about to backward...
âœ… Backward completed in 0.013s
About to run optimizer step...
âœ… Optimizer step completed in 0.187s
ðŸ”¥ Total XLA step time: 0.262s
Starting data_iter_step 15...
Calculated step = 15, num_training_steps_per_epoch = 312
Global iteration it = 15
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 15...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.015s
About to call model forward...
âœ… Model forward completed in 0.016s, about to compute loss...
âœ… Loss computed in 0.001s: 7.0625, about to backward...
âœ… Backward completed in 0.012s
About to run optimizer step...
âœ… Optimizer step completed in 0.186s
ðŸ”¥ Total XLA step time: 0.252s
Starting data_iter_step 16...
Calculated step = 16, num_training_steps_per_epoch = 312
Global iteration it = 16
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 16...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.044s
About to call model forward...
âœ… Model forward completed in 0.013s, about to compute loss...
âœ… Loss computed in 0.000s: 7.0625, about to backward...
âœ… Backward completed in 0.024s
About to run optimizer step...
âœ… Optimizer step completed in 0.197s
ðŸ”¥ Total XLA step time: 0.307s
Starting data_iter_step 17...
Calculated step = 17, num_training_steps_per_epoch = 312
Global iteration it = 17
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 17...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.045s
About to call model forward...
âœ… Model forward completed in 0.016s, about to compute loss...
âœ… Loss computed in 0.000s: 7.0, about to backward...
âœ… Backward completed in 0.023s
About to run optimizer step...
âœ… Optimizer step completed in 0.190s
ðŸ”¥ Total XLA step time: 0.309s
Starting data_iter_step 18...
Calculated step = 18, num_training_steps_per_epoch = 312
Global iteration it = 18
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 18...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.012s
About to call model forward...
âœ… Model forward completed in 0.013s, about to compute loss...
âœ… Loss computed in 0.001s: 7.0, about to backward...
âœ… Backward completed in 0.012s
About to run optimizer step...
âœ… Optimizer step completed in 0.183s
ðŸ”¥ Total XLA step time: 0.245s
Starting data_iter_step 19...
Calculated step = 19, num_training_steps_per_epoch = 312
Global iteration it = 19
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 19...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.001s
About to call model forward...
âœ… Model forward completed in 0.012s, about to compute loss...
âœ… Loss computed in 0.001s: 7.03125, about to backward...
âœ… Backward completed in 0.012s
About to run optimizer step...
âœ… Optimizer step completed in 0.185s
ðŸ”¥ Total XLA step time: 0.232s
Starting data_iter_step 20...
Calculated step = 20, num_training_steps_per_epoch = 312
Global iteration it = 20
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 20...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
âœ… Data moved to device in 0.014s
About to call model forward...
âœ… Model forward completed in 0.013s, about to compute loss...
âœ… Loss computed in 0.001s: 7.03125, about to backward...
âœ… Backward completed in 0.012s
About to run optimizer step...
âœ… Optimizer step completed in 0.185s
ðŸ”¥ Total XLA step time: 0.247s
Epoch: [0]  [ 20/312]  eta: 1:35:06  loss: 7.0625 (7.0625)  lr: 0.0000 (0.0000)  time: 17.6276  data: 15.4688
Starting data_iter_step 21...
Calculated step = 21, num_training_steps_per_epoch = 312
Global iteration it = 21
About to update LR/WD schedules...
LR/WD schedule update completed
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 21...
About to apply mixup if present...
About to enter torch_xla.step() context...
