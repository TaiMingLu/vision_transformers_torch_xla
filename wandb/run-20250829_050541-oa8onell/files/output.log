W&B online run started. URL: https://wandb.ai/ttl/tpu-vit/runs/oa8onell
TPU mode: Using num_workers=0 to avoid pickle issues, will use MpDeviceLoader for parallelism
Mixup is activated!
WARNING:timm.models._builder:No pretrained configuration specified for my_vit_b model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Forcing XLA synchronization after model.to(device)...
XLA mark_step() after model.to(device) completed
Forcing XLA synchronization before parameter counting...
XLA mark_step() completed
TPU spawned process: world_size = 32
Calculated total_batch_size = 4096
Number of training examples = 1281167
LR = 0.00400000
Batch size = 4096
Update frequent = 1
Number of training steps per epoch = 312
TPU multihost mode: XLA handles data distribution internally
About to create optimizer...
TPU mode: Using name-only parameter grouping to avoid XLA compilation
TPU mode: Collecting parameter names...
TPU mode: Found 152 trainable parameters
TPU mode: Grouping parameters by names...
Processing parameter 1/152: cls_token
Processing parameter 51/152: blocks.3.mlp.fc2.weight
Processing parameter 101/152: blocks.8.norm1.weight
Processing parameter 151/152: head.weight
TPU mode: Parameter names grouped successfully
TPU mode: Mapping names to tensors...
TPU mode: All 152 parameters mapped successfully
Parameter iteration completed! Processed 152 parameters.
TPU mode: Created 2 parameter groups, returning optimizer groups...
Parameter groups created successfully
Optimizer created successfully
TPU mode: Skipping loss scaler (XLA handles mixed precision)
TPU mode: About to set loss_scaler = None...
TPU mode: Loss scaler set to None
About to create Cosine LR scheduler...
Set warmup steps = 6240
LR scheduler created successfully
About to continue to weight decay scheduler...
About to check args.weight_decay_end...
Directly setting weight_decay_end to avoid freeze...
weight_decay_end set to: 0.05
About to call cosine_scheduler for weight decay...
Calling with: wd=0.05, wd_end=0.05, epochs=100, steps_per_epoch=312
Set warmup steps = 0
cosine_scheduler completed, about to calculate min/max...
Weight decay schedule created: 0.05 -> 0.05
About to create criterion...
Created SoftTargetCrossEntropy criterion
criterion = SoftTargetCrossEntropy()
TPU mode: Skipping warmup (caused shape errors), will compile on first iteration
About to call auto_load_model...
auto_load_model completed
About to define get_eval_model helper function...
get_eval_model function defined
About to set max_accuracy...
max_accuracy initialized
Start training for 100 epochs
About to record start_time...
About to enter training loop...
Starting epoch 0...
Setting sampler epoch...
Setting wandb steps...
About to call train_one_epoch...
train_one_epoch started: epoch=0, tpu=True
🚀 Wrapping DataLoader with MpDeviceLoader for TPU parallelism...
✅ MpDeviceLoader created successfully - enables prefetching and parallelism
About to zero_grad optimizer...
About to start data loader loop...
🚀 Starting data_iter_step 0 (DataLoader iteration took 0.000s)...
Calculated step = 0, num_training_steps_per_epoch = 312
Global iteration it = 0
TPU mode: Processing iteration 0, step 0, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 0...
TPU mode: Current device = xla:0, device type = xla
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
TPU mode: samples.device = xla:0
TPU mode: targets.device = xla:0
TPU mode: model device = xla:0
About to call model forward...
✅ Model forward completed in 0.023s, about to compute loss...
✅ Loss computed in 0.000s: 7.15625, about to backward...
✅ Backward completed in 0.025s
About to run optimizer step...
✅ Optimizer step completed in 27.008s
🔥 Total XLA step time: 40.427s
TPU mode: Iteration 0 completed in 40.43s
📝 End of iteration 0 - about to continue loop...
TPU mode: Iteration 0, TPU memory: {'bytes_used': 1697257472, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 17.070s
✅ Wandb logged in 0.000s
Epoch: [0]  [  0/312]  eta: 14:06:24  loss: 7.1562 (7.1562)  lr: 0.0000 (0.0000)  time: 162.7699  data: 105.2665
🚀 Starting data_iter_step 1 (DataLoader iteration took 17.073s)...
Calculated step = 1, num_training_steps_per_epoch = 312
Global iteration it = 1
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 1...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 29.125s
🔥 Total XLA step time: 29.555s
TPU mode: Iteration 1 completed in 29.56s
📝 End of iteration 1 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 11.395s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 2 (DataLoader iteration took 11.397s)...
Calculated step = 2, num_training_steps_per_epoch = 312
Global iteration it = 2
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 2...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.187s
🔥 Total XLA step time: 0.232s
TPU mode: Iteration 2 completed in 0.23s
📝 End of iteration 2 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.542s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 3 (DataLoader iteration took 0.543s)...
Calculated step = 3, num_training_steps_per_epoch = 312
Global iteration it = 3
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 3...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.187s
🔥 Total XLA step time: 0.231s
TPU mode: Iteration 3 completed in 0.23s
📝 End of iteration 3 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.449s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 4 (DataLoader iteration took 0.451s)...
Calculated step = 4, num_training_steps_per_epoch = 312
Global iteration it = 4
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 4...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.187s
🔥 Total XLA step time: 0.232s
TPU mode: Iteration 4 completed in 0.23s
📝 End of iteration 4 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.561s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 5 (DataLoader iteration took 0.563s)...
Calculated step = 5, num_training_steps_per_epoch = 312
Global iteration it = 5
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 5...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.188s
🔥 Total XLA step time: 0.574s
TPU mode: Iteration 5 completed in 0.58s
📝 End of iteration 5 - about to continue loop...
🚀 Starting data_iter_step 6 (DataLoader iteration took 0.000s)...
Calculated step = 6, num_training_steps_per_epoch = 312
Global iteration it = 6
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 6...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.192s
🔥 Total XLA step time: 0.571s
TPU mode: Iteration 6 completed in 0.57s
📝 End of iteration 6 - about to continue loop...
🚀 Starting data_iter_step 7 (DataLoader iteration took 0.001s)...
Calculated step = 7, num_training_steps_per_epoch = 312
Global iteration it = 7
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 7...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.268s
🔥 Total XLA step time: 0.737s
TPU mode: Iteration 7 completed in 0.74s
📝 End of iteration 7 - about to continue loop...
🚀 Starting data_iter_step 8 (DataLoader iteration took 0.000s)...
Calculated step = 8, num_training_steps_per_epoch = 312
Global iteration it = 8
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 8...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.358s
TPU mode: Iteration 8 completed in 0.36s
📝 End of iteration 8 - about to continue loop...
🚀 Starting data_iter_step 9 (DataLoader iteration took 0.000s)...
Calculated step = 9, num_training_steps_per_epoch = 312
Global iteration it = 9
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 9...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.483s
TPU mode: Iteration 9 completed in 0.48s
📝 End of iteration 9 - about to continue loop...
🚀 Starting data_iter_step 10 (DataLoader iteration took 0.000s)...
Calculated step = 10, num_training_steps_per_epoch = 312
Global iteration it = 10
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 10...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.631s
TPU mode: Iteration 10 completed in 0.63s
📝 End of iteration 10 - about to continue loop...
Epoch: [0]  [ 10/312]  eta: 1:35:47  loss: 7.0938 (7.0875)  lr: 0.0000 (0.0000)  time: 19.0324  data: 9.5700
🚀 Starting data_iter_step 11 (DataLoader iteration took 0.001s)...
Calculated step = 11, num_training_steps_per_epoch = 312
Global iteration it = 11
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 11...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.635s
TPU mode: Iteration 11 completed in 0.64s
📝 End of iteration 11 - about to continue loop...
🚀 Starting data_iter_step 12 (DataLoader iteration took 0.001s)...
Calculated step = 12, num_training_steps_per_epoch = 312
Global iteration it = 12
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 12...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.231s
🔥 Total XLA step time: 0.625s
TPU mode: Iteration 12 completed in 0.63s
📝 End of iteration 12 - about to continue loop...
🚀 Starting data_iter_step 13 (DataLoader iteration took 0.000s)...
Calculated step = 13, num_training_steps_per_epoch = 312
Global iteration it = 13
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 13...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.647s
TPU mode: Iteration 13 completed in 0.65s
📝 End of iteration 13 - about to continue loop...
🚀 Starting data_iter_step 14 (DataLoader iteration took 0.000s)...
Calculated step = 14, num_training_steps_per_epoch = 312
Global iteration it = 14
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 14...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.463s
TPU mode: Iteration 14 completed in 0.46s
📝 End of iteration 14 - about to continue loop...
🚀 Starting data_iter_step 15 (DataLoader iteration took 0.001s)...
Calculated step = 15, num_training_steps_per_epoch = 312
Global iteration it = 15
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 15...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.747s
TPU mode: Iteration 15 completed in 0.75s
📝 End of iteration 15 - about to continue loop...
🚀 Starting data_iter_step 16 (DataLoader iteration took 0.000s)...
Calculated step = 16, num_training_steps_per_epoch = 312
Global iteration it = 16
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 16...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.229s
🔥 Total XLA step time: 0.405s
TPU mode: Iteration 16 completed in 0.41s
📝 End of iteration 16 - about to continue loop...
🚀 Starting data_iter_step 17 (DataLoader iteration took 0.000s)...
Calculated step = 17, num_training_steps_per_epoch = 312
Global iteration it = 17
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 17...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.224s
🔥 Total XLA step time: 0.589s
TPU mode: Iteration 17 completed in 0.59s
📝 End of iteration 17 - about to continue loop...
🚀 Starting data_iter_step 18 (DataLoader iteration took 0.000s)...
Calculated step = 18, num_training_steps_per_epoch = 312
Global iteration it = 18
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 18...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.230s
🔥 Total XLA step time: 0.521s
TPU mode: Iteration 18 completed in 0.52s
📝 End of iteration 18 - about to continue loop...
🚀 Starting data_iter_step 19 (DataLoader iteration took 0.000s)...
Calculated step = 19, num_training_steps_per_epoch = 312
Global iteration it = 19
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 19...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.229s
🔥 Total XLA step time: 0.606s
TPU mode: Iteration 19 completed in 0.61s
📝 End of iteration 19 - about to continue loop...
🚀 Starting data_iter_step 20 (DataLoader iteration took 0.000s)...
Calculated step = 20, num_training_steps_per_epoch = 312
Global iteration it = 20
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 20...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.226s
🔥 Total XLA step time: 0.645s
Epoch: [0]  [ 20/312]  eta: 0:49:53  loss: 7.0938 (7.0875)  lr: 0.0000 (0.0000)  time: 2.6251  data: 0.0003
🚀 Starting data_iter_step 21 (DataLoader iteration took 0.001s)...
Calculated step = 21, num_training_steps_per_epoch = 312
Global iteration it = 21
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 21...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.723s
🚀 Starting data_iter_step 22 (DataLoader iteration took 0.000s)...
Calculated step = 22, num_training_steps_per_epoch = 312
Global iteration it = 22
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 22...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.231s
🔥 Total XLA step time: 0.382s
🚀 Starting data_iter_step 23 (DataLoader iteration took 0.000s)...
Calculated step = 23, num_training_steps_per_epoch = 312
Global iteration it = 23
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 23...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 1.019s
🚀 Starting data_iter_step 24 (DataLoader iteration took 0.000s)...
Calculated step = 24, num_training_steps_per_epoch = 312
Global iteration it = 24
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 24...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.253s
🔥 Total XLA step time: 0.396s
🚀 Starting data_iter_step 25 (DataLoader iteration took 0.000s)...
Calculated step = 25, num_training_steps_per_epoch = 312
Global iteration it = 25
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 25...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.771s
🚀 Starting data_iter_step 26 (DataLoader iteration took 0.000s)...
Calculated step = 26, num_training_steps_per_epoch = 312
Global iteration it = 26
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 26...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.651s
🚀 Starting data_iter_step 27 (DataLoader iteration took 0.000s)...
Calculated step = 27, num_training_steps_per_epoch = 312
Global iteration it = 27
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 27...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.577s
🚀 Starting data_iter_step 28 (DataLoader iteration took 0.000s)...
Calculated step = 28, num_training_steps_per_epoch = 312
Global iteration it = 28
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 28...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.703s
🚀 Starting data_iter_step 29 (DataLoader iteration took 0.000s)...
Calculated step = 29, num_training_steps_per_epoch = 312
Global iteration it = 29
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 29...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.660s
🚀 Starting data_iter_step 30 (DataLoader iteration took 0.000s)...
Calculated step = 30, num_training_steps_per_epoch = 312
Global iteration it = 30
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 30...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.251s
🔥 Total XLA step time: 0.670s
Epoch: [0]  [ 30/312]  eta: 0:33:38  loss: 7.0938 (7.0875)  lr: 0.0000 (0.0000)  time: 0.6248  data: 0.0003
🚀 Starting data_iter_step 31 (DataLoader iteration took 0.001s)...
Calculated step = 31, num_training_steps_per_epoch = 312
Global iteration it = 31
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 31...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.227s
🔥 Total XLA step time: 0.467s
🚀 Starting data_iter_step 32 (DataLoader iteration took 0.000s)...
Calculated step = 32, num_training_steps_per_epoch = 312
Global iteration it = 32
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 32...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.230s
🔥 Total XLA step time: 0.675s
🚀 Starting data_iter_step 33 (DataLoader iteration took 0.000s)...
Calculated step = 33, num_training_steps_per_epoch = 312
Global iteration it = 33
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 33...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.637s
🚀 Starting data_iter_step 34 (DataLoader iteration took 0.000s)...
Calculated step = 34, num_training_steps_per_epoch = 312
Global iteration it = 34
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 34...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.488s
🚀 Starting data_iter_step 35 (DataLoader iteration took 0.000s)...
Calculated step = 35, num_training_steps_per_epoch = 312
Global iteration it = 35
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 35...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.668s
🚀 Starting data_iter_step 36 (DataLoader iteration took 0.000s)...
Calculated step = 36, num_training_steps_per_epoch = 312
Global iteration it = 36
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 36...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.768s
🚀 Starting data_iter_step 37 (DataLoader iteration took 0.000s)...
Calculated step = 37, num_training_steps_per_epoch = 312
Global iteration it = 37
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 37...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.560s
🚀 Starting data_iter_step 38 (DataLoader iteration took 0.000s)...
Calculated step = 38, num_training_steps_per_epoch = 312
Global iteration it = 38
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 38...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.703s
🚀 Starting data_iter_step 39 (DataLoader iteration took 0.001s)...
Calculated step = 39, num_training_steps_per_epoch = 312
Global iteration it = 39
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 39...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.231s
🔥 Total XLA step time: 0.673s
🚀 Starting data_iter_step 40 (DataLoader iteration took 0.000s)...
Calculated step = 40, num_training_steps_per_epoch = 312
Global iteration it = 40
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 40...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.680s
Epoch: [0]  [ 40/312]  eta: 0:25:13  loss: 7.0938 (7.0875)  lr: 0.0000 (0.0000)  time: 0.6464  data: 0.0002
🚀 Starting data_iter_step 41 (DataLoader iteration took 0.001s)...
Calculated step = 41, num_training_steps_per_epoch = 312
Global iteration it = 41
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 41...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.229s
🔥 Total XLA step time: 0.620s
🚀 Starting data_iter_step 42 (DataLoader iteration took 0.000s)...
Calculated step = 42, num_training_steps_per_epoch = 312
Global iteration it = 42
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 42...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.231s
🔥 Total XLA step time: 0.612s
🚀 Starting data_iter_step 43 (DataLoader iteration took 0.000s)...
Calculated step = 43, num_training_steps_per_epoch = 312
Global iteration it = 43
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 43...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.222s
🔥 Total XLA step time: 0.336s
🚀 Starting data_iter_step 44 (DataLoader iteration took 0.000s)...
Calculated step = 44, num_training_steps_per_epoch = 312
Global iteration it = 44
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 44...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.230s
🔥 Total XLA step time: 0.598s
🚀 Starting data_iter_step 45 (DataLoader iteration took 0.000s)...
Calculated step = 45, num_training_steps_per_epoch = 312
Global iteration it = 45
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 45...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.675s
🚀 Starting data_iter_step 46 (DataLoader iteration took 0.000s)...
Calculated step = 46, num_training_steps_per_epoch = 312
Global iteration it = 46
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 46...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.716s
🚀 Starting data_iter_step 47 (DataLoader iteration took 0.000s)...
Calculated step = 47, num_training_steps_per_epoch = 312
Global iteration it = 47
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 47...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.231s
🔥 Total XLA step time: 0.550s
🚀 Starting data_iter_step 48 (DataLoader iteration took 0.000s)...
Calculated step = 48, num_training_steps_per_epoch = 312
Global iteration it = 48
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 48...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.231s
🔥 Total XLA step time: 0.648s
🚀 Starting data_iter_step 49 (DataLoader iteration took 0.000s)...
Calculated step = 49, num_training_steps_per_epoch = 312
Global iteration it = 49
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 49...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.230s
🔥 Total XLA step time: 0.589s
🚀 Starting data_iter_step 50 (DataLoader iteration took 0.000s)...
Calculated step = 50, num_training_steps_per_epoch = 312
Global iteration it = 50
TPU mode: Processing iteration 50, step 50, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 50...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.000s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.226s
🔥 Total XLA step time: 0.762s
TPU mode: Iteration 50, TPU memory: {'bytes_used': 1805008384, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 0.333s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 50/312]  eta: 0:20:05  loss: 7.0625 (7.0781)  lr: 0.0000 (0.0000)  time: 0.6407  data: 0.0002
🚀 Starting data_iter_step 51 (DataLoader iteration took 0.335s)...
Calculated step = 51, num_training_steps_per_epoch = 312
Global iteration it = 51
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 51...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.231s
🔥 Total XLA step time: 0.278s
🚀 Starting data_iter_step 52 (DataLoader iteration took 0.000s)...
Calculated step = 52, num_training_steps_per_epoch = 312
Global iteration it = 52
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 52...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.225s
🔥 Total XLA step time: 0.860s
🚀 Starting data_iter_step 53 (DataLoader iteration took 0.000s)...
Calculated step = 53, num_training_steps_per_epoch = 312
Global iteration it = 53
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 53...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.228s
🔥 Total XLA step time: 0.824s
🚀 Starting data_iter_step 54 (DataLoader iteration took 0.000s)...
Calculated step = 54, num_training_steps_per_epoch = 312
Global iteration it = 54
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 54...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.229s
🔥 Total XLA step time: 0.370s
🚀 Starting data_iter_step 55 (DataLoader iteration took 0.000s)...
Calculated step = 55, num_training_steps_per_epoch = 312
Global iteration it = 55
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 55...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.228s
🔥 Total XLA step time: 0.700s
🚀 Starting data_iter_step 56 (DataLoader iteration took 0.000s)...
Calculated step = 56, num_training_steps_per_epoch = 312
Global iteration it = 56
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 56...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 7.09375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.230s
🔥 Total XLA step time: 0.718s
🚀 Starting data_iter_step 57 (DataLoader iteration took 0.000s)...
Calculated step = 57, num_training_steps_per_epoch = 312
Global iteration it = 57
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 57...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.639s
🚀 Starting data_iter_step 58 (DataLoader iteration took 0.000s)...
Calculated step = 58, num_training_steps_per_epoch = 312
Global iteration it = 58
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 58...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.676s
🚀 Starting data_iter_step 59 (DataLoader iteration took 0.000s)...
Calculated step = 59, num_training_steps_per_epoch = 312
Global iteration it = 59
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 59...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.226s
🔥 Total XLA step time: 0.623s
🚀 Starting data_iter_step 60 (DataLoader iteration took 0.000s)...
Calculated step = 60, num_training_steps_per_epoch = 312
Global iteration it = 60
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 60...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.230s
🔥 Total XLA step time: 0.674s
Epoch: [0]  [ 60/312]  eta: 0:16:35  loss: 7.0625 (7.0781)  lr: 0.0000 (0.0000)  time: 0.6427  data: 0.0002
🚀 Starting data_iter_step 61 (DataLoader iteration took 0.001s)...
Calculated step = 61, num_training_steps_per_epoch = 312
Global iteration it = 61
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 61...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.447s
🚀 Starting data_iter_step 62 (DataLoader iteration took 0.000s)...
Calculated step = 62, num_training_steps_per_epoch = 312
Global iteration it = 62
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 62...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.565s
🚀 Starting data_iter_step 63 (DataLoader iteration took 0.000s)...
Calculated step = 63, num_training_steps_per_epoch = 312
Global iteration it = 63
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 63...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.228s
🔥 Total XLA step time: 0.627s
🚀 Starting data_iter_step 64 (DataLoader iteration took 0.000s)...
Calculated step = 64, num_training_steps_per_epoch = 312
Global iteration it = 64
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 64...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.508s
🚀 Starting data_iter_step 65 (DataLoader iteration took 0.000s)...
Calculated step = 65, num_training_steps_per_epoch = 312
Global iteration it = 65
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 65...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.231s
🔥 Total XLA step time: 0.631s
🚀 Starting data_iter_step 66 (DataLoader iteration took 0.000s)...
Calculated step = 66, num_training_steps_per_epoch = 312
Global iteration it = 66
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 66...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.230s
🔥 Total XLA step time: 0.627s
🚀 Starting data_iter_step 67 (DataLoader iteration took 0.000s)...
Calculated step = 67, num_training_steps_per_epoch = 312
Global iteration it = 67
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 67...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.230s
🔥 Total XLA step time: 0.549s
🚀 Starting data_iter_step 68 (DataLoader iteration took 0.000s)...
Calculated step = 68, num_training_steps_per_epoch = 312
Global iteration it = 68
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 68...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.576s
🚀 Starting data_iter_step 69 (DataLoader iteration took 0.000s)...
Calculated step = 69, num_training_steps_per_epoch = 312
Global iteration it = 69
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 69...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.017s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.617s
🚀 Starting data_iter_step 70 (DataLoader iteration took 0.000s)...
Calculated step = 70, num_training_steps_per_epoch = 312
Global iteration it = 70
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 70...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.699s
Epoch: [0]  [ 70/312]  eta: 0:14:01  loss: 7.0625 (7.0781)  lr: 0.0000 (0.0000)  time: 0.6128  data: 0.0002
🚀 Starting data_iter_step 71 (DataLoader iteration took 0.001s)...
Calculated step = 71, num_training_steps_per_epoch = 312
Global iteration it = 71
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 71...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.637s
🚀 Starting data_iter_step 72 (DataLoader iteration took 0.000s)...
Calculated step = 72, num_training_steps_per_epoch = 312
Global iteration it = 72
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 72...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.654s
🚀 Starting data_iter_step 73 (DataLoader iteration took 0.000s)...
Calculated step = 73, num_training_steps_per_epoch = 312
Global iteration it = 73
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 73...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.017s
About to run optimizer step...
✅ Optimizer step completed in 0.247s
🔥 Total XLA step time: 0.540s
🚀 Starting data_iter_step 74 (DataLoader iteration took 0.000s)...
Calculated step = 74, num_training_steps_per_epoch = 312
Global iteration it = 74
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 74...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.629s
🚀 Starting data_iter_step 75 (DataLoader iteration took 0.000s)...
Calculated step = 75, num_training_steps_per_epoch = 312
Global iteration it = 75
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 75...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.517s
🚀 Starting data_iter_step 76 (DataLoader iteration took 0.000s)...
Calculated step = 76, num_training_steps_per_epoch = 312
Global iteration it = 76
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 76...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.784s
🚀 Starting data_iter_step 77 (DataLoader iteration took 0.000s)...
Calculated step = 77, num_training_steps_per_epoch = 312
Global iteration it = 77
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 77...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.251s
🔥 Total XLA step time: 0.566s
🚀 Starting data_iter_step 78 (DataLoader iteration took 0.000s)...
Calculated step = 78, num_training_steps_per_epoch = 312
Global iteration it = 78
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 78...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.629s
🚀 Starting data_iter_step 79 (DataLoader iteration took 0.000s)...
Calculated step = 79, num_training_steps_per_epoch = 312
Global iteration it = 79
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 79...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.650s
🚀 Starting data_iter_step 80 (DataLoader iteration took 0.000s)...
Calculated step = 80, num_training_steps_per_epoch = 312
Global iteration it = 80
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 80...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.651s
Epoch: [0]  [ 80/312]  eta: 0:12:05  loss: 7.0625 (7.0781)  lr: 0.0000 (0.0000)  time: 0.6076  data: 0.0002
🚀 Starting data_iter_step 81 (DataLoader iteration took 0.001s)...
Calculated step = 81, num_training_steps_per_epoch = 312
Global iteration it = 81
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 81...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.247s
🔥 Total XLA step time: 0.662s
🚀 Starting data_iter_step 82 (DataLoader iteration took 0.000s)...
Calculated step = 82, num_training_steps_per_epoch = 312
Global iteration it = 82
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 82...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.523s
🚀 Starting data_iter_step 83 (DataLoader iteration took 0.000s)...
Calculated step = 83, num_training_steps_per_epoch = 312
Global iteration it = 83
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 83...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.639s
🚀 Starting data_iter_step 84 (DataLoader iteration took 0.000s)...
Calculated step = 84, num_training_steps_per_epoch = 312
Global iteration it = 84
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 84...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.624s
🚀 Starting data_iter_step 85 (DataLoader iteration took 0.000s)...
Calculated step = 85, num_training_steps_per_epoch = 312
Global iteration it = 85
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 85...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.550s
🚀 Starting data_iter_step 86 (DataLoader iteration took 0.000s)...
Calculated step = 86, num_training_steps_per_epoch = 312
Global iteration it = 86
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 86...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.676s
🚀 Starting data_iter_step 87 (DataLoader iteration took 0.000s)...
Calculated step = 87, num_training_steps_per_epoch = 312
Global iteration it = 87
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 87...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.763s
🚀 Starting data_iter_step 88 (DataLoader iteration took 1.368s)...
Calculated step = 88, num_training_steps_per_epoch = 312
Global iteration it = 88
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 88...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.730s
🚀 Starting data_iter_step 89 (DataLoader iteration took 0.000s)...
Calculated step = 89, num_training_steps_per_epoch = 312
Global iteration it = 89
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 89...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.800s
🚀 Starting data_iter_step 90 (DataLoader iteration took 0.000s)...
Calculated step = 90, num_training_steps_per_epoch = 312
Global iteration it = 90
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 90...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.643s
Epoch: [0]  [ 90/312]  eta: 0:10:37  loss: 7.0625 (7.0781)  lr: 0.0000 (0.0000)  time: 0.7146  data: 0.0686
🚀 Starting data_iter_step 91 (DataLoader iteration took 0.001s)...
Calculated step = 91, num_training_steps_per_epoch = 312
Global iteration it = 91
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 91...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.636s
🚀 Starting data_iter_step 92 (DataLoader iteration took 0.000s)...
Calculated step = 92, num_training_steps_per_epoch = 312
Global iteration it = 92
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 92...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.011s, about to compute loss...
✅ Loss computed in 0.000s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.638s
🚀 Starting data_iter_step 93 (DataLoader iteration took 0.000s)...
Calculated step = 93, num_training_steps_per_epoch = 312
Global iteration it = 93
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 93...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.540s
🚀 Starting data_iter_step 94 (DataLoader iteration took 0.000s)...
Calculated step = 94, num_training_steps_per_epoch = 312
Global iteration it = 94
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 94...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.582s
🚀 Starting data_iter_step 95 (DataLoader iteration took 0.000s)...
Calculated step = 95, num_training_steps_per_epoch = 312
Global iteration it = 95
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 95...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.595s
🚀 Starting data_iter_step 96 (DataLoader iteration took 1.368s)...
Calculated step = 96, num_training_steps_per_epoch = 312
Global iteration it = 96
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 96...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.230s
🔥 Total XLA step time: 0.689s
🚀 Starting data_iter_step 97 (DataLoader iteration took 0.000s)...
Calculated step = 97, num_training_steps_per_epoch = 312
Global iteration it = 97
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 97...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.777s
🚀 Starting data_iter_step 98 (DataLoader iteration took 0.000s)...
Calculated step = 98, num_training_steps_per_epoch = 312
Global iteration it = 98
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 98...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.625s
🚀 Starting data_iter_step 99 (DataLoader iteration took 0.000s)...
Calculated step = 99, num_training_steps_per_epoch = 312
Global iteration it = 99
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 99...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.620s
🚀 Starting data_iter_step 100 (DataLoader iteration took 0.001s)...
Calculated step = 100, num_training_steps_per_epoch = 312
Global iteration it = 100
TPU mode: Processing iteration 100, step 100, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 100...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.229s
🔥 Total XLA step time: 0.554s
TPU mode: Iteration 100, TPU memory: {'bytes_used': 1580534272, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 0.336s
✅ Wandb logged in 0.000s
Epoch: [0]  [100/312]  eta: 0:09:25  loss: 7.0625 (7.0670)  lr: 0.0000 (0.0000)  time: 0.7999  data: 0.1370
🚀 Starting data_iter_step 101 (DataLoader iteration took 0.339s)...
Calculated step = 101, num_training_steps_per_epoch = 312
Global iteration it = 101
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 101...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.633s
🚀 Starting data_iter_step 102 (DataLoader iteration took 0.000s)...
Calculated step = 102, num_training_steps_per_epoch = 312
Global iteration it = 102
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 102...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.414s
🚀 Starting data_iter_step 103 (DataLoader iteration took 0.000s)...
Calculated step = 103, num_training_steps_per_epoch = 312
Global iteration it = 103
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 103...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.675s
🚀 Starting data_iter_step 104 (DataLoader iteration took 0.984s)...
Calculated step = 104, num_training_steps_per_epoch = 312
Global iteration it = 104
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 104...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.775s
🚀 Starting data_iter_step 105 (DataLoader iteration took 0.000s)...
Calculated step = 105, num_training_steps_per_epoch = 312
Global iteration it = 105
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 105...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.749s
🚀 Starting data_iter_step 106 (DataLoader iteration took 0.000s)...
Calculated step = 106, num_training_steps_per_epoch = 312
Global iteration it = 106
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 106...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.638s
🚀 Starting data_iter_step 107 (DataLoader iteration took 0.000s)...
Calculated step = 107, num_training_steps_per_epoch = 312
Global iteration it = 107
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 107...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.470s
🚀 Starting data_iter_step 108 (DataLoader iteration took 0.000s)...
Calculated step = 108, num_training_steps_per_epoch = 312
Global iteration it = 108
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 108...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.784s
🚀 Starting data_iter_step 109 (DataLoader iteration took 0.000s)...
Calculated step = 109, num_training_steps_per_epoch = 312
Global iteration it = 109
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 109...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.570s
🚀 Starting data_iter_step 110 (DataLoader iteration took 0.000s)...
Calculated step = 110, num_training_steps_per_epoch = 312
Global iteration it = 110
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 110...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.597s
Epoch: [0]  [110/312]  eta: 0:08:23  loss: 7.0625 (7.0670)  lr: 0.0000 (0.0000)  time: 0.7653  data: 0.1178
🚀 Starting data_iter_step 111 (DataLoader iteration took 0.001s)...
Calculated step = 111, num_training_steps_per_epoch = 312
Global iteration it = 111
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 111...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.710s
🚀 Starting data_iter_step 112 (DataLoader iteration took 0.987s)...
Calculated step = 112, num_training_steps_per_epoch = 312
Global iteration it = 112
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 112...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.645s
🚀 Starting data_iter_step 113 (DataLoader iteration took 0.000s)...
Calculated step = 113, num_training_steps_per_epoch = 312
Global iteration it = 113
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 113...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 1.295s
🚀 Starting data_iter_step 114 (DataLoader iteration took 0.000s)...
Calculated step = 114, num_training_steps_per_epoch = 312
Global iteration it = 114
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 114...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.639s
🚀 Starting data_iter_step 115 (DataLoader iteration took 0.000s)...
Calculated step = 115, num_training_steps_per_epoch = 312
Global iteration it = 115
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 115...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.628s
🚀 Starting data_iter_step 116 (DataLoader iteration took 0.000s)...
Calculated step = 116, num_training_steps_per_epoch = 312
Global iteration it = 116
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 116...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.739s
🚀 Starting data_iter_step 117 (DataLoader iteration took 0.000s)...
Calculated step = 117, num_training_steps_per_epoch = 312
Global iteration it = 117
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 117...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.354s
🚀 Starting data_iter_step 118 (DataLoader iteration took 0.000s)...
Calculated step = 118, num_training_steps_per_epoch = 312
Global iteration it = 118
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 118...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.697s
🚀 Starting data_iter_step 119 (DataLoader iteration took 0.000s)...
Calculated step = 119, num_training_steps_per_epoch = 312
Global iteration it = 119
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 119...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.251s
🔥 Total XLA step time: 0.634s
🚀 Starting data_iter_step 120 (DataLoader iteration took 0.808s)...
Calculated step = 120, num_training_steps_per_epoch = 312
Global iteration it = 120
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 120...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.291s
Epoch: [0]  [120/312]  eta: 0:07:32  loss: 7.0625 (7.0670)  lr: 0.0000 (0.0000)  time: 0.7886  data: 0.1391
🚀 Starting data_iter_step 121 (DataLoader iteration took 0.001s)...
Calculated step = 121, num_training_steps_per_epoch = 312
Global iteration it = 121
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 121...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 1.566s
🚀 Starting data_iter_step 122 (DataLoader iteration took 0.000s)...
Calculated step = 122, num_training_steps_per_epoch = 312
Global iteration it = 122
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 122...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.753s
🚀 Starting data_iter_step 123 (DataLoader iteration took 0.000s)...
Calculated step = 123, num_training_steps_per_epoch = 312
Global iteration it = 123
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 123...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.596s
🚀 Starting data_iter_step 124 (DataLoader iteration took 0.000s)...
Calculated step = 124, num_training_steps_per_epoch = 312
Global iteration it = 124
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 124...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.473s
🚀 Starting data_iter_step 125 (DataLoader iteration took 0.000s)...
Calculated step = 125, num_training_steps_per_epoch = 312
Global iteration it = 125
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 125...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.617s
🚀 Starting data_iter_step 126 (DataLoader iteration took 0.000s)...
Calculated step = 126, num_training_steps_per_epoch = 312
Global iteration it = 126
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 126...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.632s
🚀 Starting data_iter_step 127 (DataLoader iteration took 0.000s)...
Calculated step = 127, num_training_steps_per_epoch = 312
Global iteration it = 127
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 127...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.541s
🚀 Starting data_iter_step 128 (DataLoader iteration took 0.764s)...
Calculated step = 128, num_training_steps_per_epoch = 312
Global iteration it = 128
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 128...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.296s
🚀 Starting data_iter_step 129 (DataLoader iteration took 0.000s)...
Calculated step = 129, num_training_steps_per_epoch = 312
Global iteration it = 129
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 129...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 1.782s
🚀 Starting data_iter_step 130 (DataLoader iteration took 0.000s)...
Calculated step = 130, num_training_steps_per_epoch = 312
Global iteration it = 130
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 130...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.017s
About to run optimizer step...
✅ Optimizer step completed in 0.250s
🔥 Total XLA step time: 0.608s
Epoch: [0]  [130/312]  eta: 0:06:47  loss: 7.0625 (7.0670)  lr: 0.0000 (0.0000)  time: 0.8555  data: 0.1281
🚀 Starting data_iter_step 131 (DataLoader iteration took 0.001s)...
Calculated step = 131, num_training_steps_per_epoch = 312
Global iteration it = 131
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 131...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.469s
🚀 Starting data_iter_step 132 (DataLoader iteration took 0.000s)...
Calculated step = 132, num_training_steps_per_epoch = 312
Global iteration it = 132
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 132...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.247s
🔥 Total XLA step time: 0.688s
🚀 Starting data_iter_step 133 (DataLoader iteration took 0.000s)...
Calculated step = 133, num_training_steps_per_epoch = 312
Global iteration it = 133
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 133...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.633s
🚀 Starting data_iter_step 134 (DataLoader iteration took 0.000s)...
Calculated step = 134, num_training_steps_per_epoch = 312
Global iteration it = 134
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 134...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.227s
🔥 Total XLA step time: 0.614s
🚀 Starting data_iter_step 135 (DataLoader iteration took 0.000s)...
Calculated step = 135, num_training_steps_per_epoch = 312
Global iteration it = 135
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 135...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.534s
🚀 Starting data_iter_step 136 (DataLoader iteration took 0.766s)...
Calculated step = 136, num_training_steps_per_epoch = 312
Global iteration it = 136
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 136...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.594s
🚀 Starting data_iter_step 137 (DataLoader iteration took 0.000s)...
Calculated step = 137, num_training_steps_per_epoch = 312
Global iteration it = 137
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 137...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 1.396s
🚀 Starting data_iter_step 138 (DataLoader iteration took 0.000s)...
Calculated step = 138, num_training_steps_per_epoch = 312
Global iteration it = 138
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 138...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.616s
🚀 Starting data_iter_step 139 (DataLoader iteration took 0.000s)...
Calculated step = 139, num_training_steps_per_epoch = 312
Global iteration it = 139
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 139...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.582s
🚀 Starting data_iter_step 140 (DataLoader iteration took 0.000s)...
Calculated step = 140, num_training_steps_per_epoch = 312
Global iteration it = 140
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 140...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.636s
Epoch: [0]  [140/312]  eta: 0:06:07  loss: 7.0625 (7.0670)  lr: 0.0000 (0.0000)  time: 0.8107  data: 0.0767
🚀 Starting data_iter_step 141 (DataLoader iteration took 0.001s)...
Calculated step = 141, num_training_steps_per_epoch = 312
Global iteration it = 141
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 141...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.472s
🚀 Starting data_iter_step 142 (DataLoader iteration took 0.000s)...
Calculated step = 142, num_training_steps_per_epoch = 312
Global iteration it = 142
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 142...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.020s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.262s
🔥 Total XLA step time: 0.532s
🚀 Starting data_iter_step 143 (DataLoader iteration took 0.000s)...
Calculated step = 143, num_training_steps_per_epoch = 312
Global iteration it = 143
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 143...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.645s
🚀 Starting data_iter_step 144 (DataLoader iteration took 0.886s)...
Calculated step = 144, num_training_steps_per_epoch = 312
Global iteration it = 144
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 144...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.294s
🚀 Starting data_iter_step 145 (DataLoader iteration took 0.000s)...
Calculated step = 145, num_training_steps_per_epoch = 312
Global iteration it = 145
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 145...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 2.066s
🚀 Starting data_iter_step 146 (DataLoader iteration took 0.000s)...
Calculated step = 146, num_training_steps_per_epoch = 312
Global iteration it = 146
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 146...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.251s
🔥 Total XLA step time: 0.712s
🚀 Starting data_iter_step 147 (DataLoader iteration took 0.000s)...
Calculated step = 147, num_training_steps_per_epoch = 312
Global iteration it = 147
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 147...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.760s
🚀 Starting data_iter_step 148 (DataLoader iteration took 0.000s)...
Calculated step = 148, num_training_steps_per_epoch = 312
Global iteration it = 148
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 148...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.620s
🚀 Starting data_iter_step 149 (DataLoader iteration took 0.000s)...
Calculated step = 149, num_training_steps_per_epoch = 312
Global iteration it = 149
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 149...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.268s
🔥 Total XLA step time: 0.384s
🚀 Starting data_iter_step 150 (DataLoader iteration took 0.000s)...
Calculated step = 150, num_training_steps_per_epoch = 312
Global iteration it = 150
TPU mode: Processing iteration 150, step 150, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 150...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.462s
TPU mode: Iteration 150, TPU memory: {'bytes_used': 1588652544, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 0.280s
✅ Wandb logged in 0.000s
Epoch: [0]  [150/312]  eta: 0:05:31  loss: 7.0312 (7.0508)  lr: 0.0000 (0.0000)  time: 0.7850  data: 0.0828
🚀 Starting data_iter_step 151 (DataLoader iteration took 0.282s)...
Calculated step = 151, num_training_steps_per_epoch = 312
Global iteration it = 151
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 151...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.292s
🚀 Starting data_iter_step 152 (DataLoader iteration took 0.548s)...
Calculated step = 152, num_training_steps_per_epoch = 312
Global iteration it = 152
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 152...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.295s
🚀 Starting data_iter_step 153 (DataLoader iteration took 0.000s)...
Calculated step = 153, num_training_steps_per_epoch = 312
Global iteration it = 153
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 153...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 2.258s
🚀 Starting data_iter_step 154 (DataLoader iteration took 0.000s)...
Calculated step = 154, num_training_steps_per_epoch = 312
Global iteration it = 154
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 154...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.636s
🚀 Starting data_iter_step 155 (DataLoader iteration took 0.000s)...
Calculated step = 155, num_training_steps_per_epoch = 312
Global iteration it = 155
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 155...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.994s
🚀 Starting data_iter_step 156 (DataLoader iteration took 0.000s)...
Calculated step = 156, num_training_steps_per_epoch = 312
Global iteration it = 156
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 156...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.352s
🚀 Starting data_iter_step 157 (DataLoader iteration took 0.000s)...
Calculated step = 157, num_training_steps_per_epoch = 312
Global iteration it = 157
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 157...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.784s
🚀 Starting data_iter_step 158 (DataLoader iteration took 0.000s)...
Calculated step = 158, num_training_steps_per_epoch = 312
Global iteration it = 158
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 158...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.632s
🚀 Starting data_iter_step 159 (DataLoader iteration took 0.001s)...
Calculated step = 159, num_training_steps_per_epoch = 312
Global iteration it = 159
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 159...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.685s
🚀 Starting data_iter_step 160 (DataLoader iteration took 0.000s)...
Calculated step = 160, num_training_steps_per_epoch = 312
Global iteration it = 160
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 160...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.247s
🔥 Total XLA step time: 0.360s
Epoch: [0]  [160/312]  eta: 0:04:59  loss: 7.0312 (7.0508)  lr: 0.0000 (0.0000)  time: 0.8004  data: 0.0719
🚀 Starting data_iter_step 161 (DataLoader iteration took 0.001s)...
Calculated step = 161, num_training_steps_per_epoch = 312
Global iteration it = 161
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 161...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 2.011s
🚀 Starting data_iter_step 162 (DataLoader iteration took 0.000s)...
Calculated step = 162, num_training_steps_per_epoch = 312
Global iteration it = 162
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 162...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.592s
🚀 Starting data_iter_step 163 (DataLoader iteration took 0.000s)...
Calculated step = 163, num_training_steps_per_epoch = 312
Global iteration it = 163
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 163...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.744s
🚀 Starting data_iter_step 164 (DataLoader iteration took 0.000s)...
Calculated step = 164, num_training_steps_per_epoch = 312
Global iteration it = 164
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 164...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.017s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.629s
🚀 Starting data_iter_step 165 (DataLoader iteration took 0.000s)...
Calculated step = 165, num_training_steps_per_epoch = 312
Global iteration it = 165
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 165...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 6.9375, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.420s
🚀 Starting data_iter_step 166 (DataLoader iteration took 0.000s)...
Calculated step = 166, num_training_steps_per_epoch = 312
Global iteration it = 166
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 166...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.595s
🚀 Starting data_iter_step 167 (DataLoader iteration took 0.000s)...
Calculated step = 167, num_training_steps_per_epoch = 312
Global iteration it = 167
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 167...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.017s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.508s
🚀 Starting data_iter_step 168 (DataLoader iteration took 0.512s)...
Calculated step = 168, num_training_steps_per_epoch = 312
Global iteration it = 168
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 168...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.257s
🔥 Total XLA step time: 0.612s
🚀 Starting data_iter_step 169 (DataLoader iteration took 0.000s)...
Calculated step = 169, num_training_steps_per_epoch = 312
Global iteration it = 169
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 169...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 1.506s
🚀 Starting data_iter_step 170 (DataLoader iteration took 0.000s)...
Calculated step = 170, num_training_steps_per_epoch = 312
Global iteration it = 170
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 170...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.251s
🔥 Total XLA step time: 0.753s
Epoch: [0]  [170/312]  eta: 0:04:30  loss: 7.0312 (7.0508)  lr: 0.0000 (0.0000)  time: 0.8389  data: 0.0532
🚀 Starting data_iter_step 171 (DataLoader iteration took 0.001s)...
Calculated step = 171, num_training_steps_per_epoch = 312
Global iteration it = 171
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 171...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.487s
🚀 Starting data_iter_step 172 (DataLoader iteration took 0.000s)...
Calculated step = 172, num_training_steps_per_epoch = 312
Global iteration it = 172
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 172...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.635s
🚀 Starting data_iter_step 173 (DataLoader iteration took 0.000s)...
Calculated step = 173, num_training_steps_per_epoch = 312
Global iteration it = 173
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 173...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.615s
🚀 Starting data_iter_step 174 (DataLoader iteration took 0.000s)...
Calculated step = 174, num_training_steps_per_epoch = 312
Global iteration it = 174
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 174...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.237s
🔥 Total XLA step time: 0.628s
🚀 Starting data_iter_step 175 (DataLoader iteration took 0.000s)...
Calculated step = 175, num_training_steps_per_epoch = 312
Global iteration it = 175
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 175...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.635s
🚀 Starting data_iter_step 176 (DataLoader iteration took 0.618s)...
Calculated step = 176, num_training_steps_per_epoch = 312
Global iteration it = 176
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 176...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.257s
🔥 Total XLA step time: 0.653s
🚀 Starting data_iter_step 177 (DataLoader iteration took 0.000s)...
Calculated step = 177, num_training_steps_per_epoch = 312
Global iteration it = 177
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 177...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 1.582s
🚀 Starting data_iter_step 178 (DataLoader iteration took 0.000s)...
Calculated step = 178, num_training_steps_per_epoch = 312
Global iteration it = 178
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 178...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.559s
🚀 Starting data_iter_step 179 (DataLoader iteration took 0.000s)...
Calculated step = 179, num_training_steps_per_epoch = 312
Global iteration it = 179
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 179...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.644s
🚀 Starting data_iter_step 180 (DataLoader iteration took 0.000s)...
Calculated step = 180, num_training_steps_per_epoch = 312
Global iteration it = 180
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 180...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.733s
Epoch: [0]  [180/312]  eta: 0:04:03  loss: 7.0312 (7.0508)  lr: 0.0000 (0.0000)  time: 0.8366  data: 0.0567
🚀 Starting data_iter_step 181 (DataLoader iteration took 0.001s)...
Calculated step = 181, num_training_steps_per_epoch = 312
Global iteration it = 181
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 181...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.574s
🚀 Starting data_iter_step 182 (DataLoader iteration took 0.001s)...
Calculated step = 182, num_training_steps_per_epoch = 312
Global iteration it = 182
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 182...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.253s
🔥 Total XLA step time: 0.652s
🚀 Starting data_iter_step 183 (DataLoader iteration took 0.000s)...
Calculated step = 183, num_training_steps_per_epoch = 312
Global iteration it = 183
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 183...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.438s
🚀 Starting data_iter_step 184 (DataLoader iteration took 0.437s)...
Calculated step = 184, num_training_steps_per_epoch = 312
Global iteration it = 184
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 184...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.259s
🔥 Total XLA step time: 0.657s
🚀 Starting data_iter_step 185 (DataLoader iteration took 0.000s)...
Calculated step = 185, num_training_steps_per_epoch = 312
Global iteration it = 185
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 185...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 1.776s
🚀 Starting data_iter_step 186 (DataLoader iteration took 0.000s)...
Calculated step = 186, num_training_steps_per_epoch = 312
Global iteration it = 186
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 186...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.587s
🚀 Starting data_iter_step 187 (DataLoader iteration took 0.000s)...
Calculated step = 187, num_training_steps_per_epoch = 312
Global iteration it = 187
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 187...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.563s
🚀 Starting data_iter_step 188 (DataLoader iteration took 0.000s)...
Calculated step = 188, num_training_steps_per_epoch = 312
Global iteration it = 188
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 188...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 0.647s
🚀 Starting data_iter_step 189 (DataLoader iteration took 0.000s)...
Calculated step = 189, num_training_steps_per_epoch = 312
Global iteration it = 189
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 189...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.649s
🚀 Starting data_iter_step 190 (DataLoader iteration took 0.000s)...
Calculated step = 190, num_training_steps_per_epoch = 312
Global iteration it = 190
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 190...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.261s
🔥 Total XLA step time: 0.613s
Epoch: [0]  [190/312]  eta: 0:03:38  loss: 7.0312 (7.0508)  lr: 0.0000 (0.0000)  time: 0.7721  data: 0.0530
🚀 Starting data_iter_step 191 (DataLoader iteration took 0.001s)...
Calculated step = 191, num_training_steps_per_epoch = 312
Global iteration it = 191
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 191...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.724s
🚀 Starting data_iter_step 192 (DataLoader iteration took 0.249s)...
Calculated step = 192, num_training_steps_per_epoch = 312
Global iteration it = 192
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 192...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.023s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.299s
🚀 Starting data_iter_step 193 (DataLoader iteration took 0.000s)...
Calculated step = 193, num_training_steps_per_epoch = 312
Global iteration it = 193
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 193...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 2.003s
🚀 Starting data_iter_step 194 (DataLoader iteration took 0.000s)...
Calculated step = 194, num_training_steps_per_epoch = 312
Global iteration it = 194
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 194...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.611s
🚀 Starting data_iter_step 195 (DataLoader iteration took 0.000s)...
Calculated step = 195, num_training_steps_per_epoch = 312
Global iteration it = 195
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 195...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.621s
🚀 Starting data_iter_step 196 (DataLoader iteration took 0.000s)...
Calculated step = 196, num_training_steps_per_epoch = 312
Global iteration it = 196
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 196...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.638s
🚀 Starting data_iter_step 197 (DataLoader iteration took 0.001s)...
Calculated step = 197, num_training_steps_per_epoch = 312
Global iteration it = 197
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 197...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.661s
🚀 Starting data_iter_step 198 (DataLoader iteration took 0.000s)...
Calculated step = 198, num_training_steps_per_epoch = 312
Global iteration it = 198
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 198...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.601s
🚀 Starting data_iter_step 199 (DataLoader iteration took 0.000s)...
Calculated step = 199, num_training_steps_per_epoch = 312
Global iteration it = 199
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 199...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.404s
🚀 Starting data_iter_step 200 (DataLoader iteration took 0.569s)...
Calculated step = 200, num_training_steps_per_epoch = 312
Global iteration it = 200
TPU mode: Processing iteration 200, step 200, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 200...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.572s
TPU mode: Iteration 200, TPU memory: {'bytes_used': 1600401408, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 1.670s
✅ Wandb logged in 0.000s
Epoch: [0]  [200/312]  eta: 0:03:15  loss: 7.0312 (7.0417)  lr: 0.0000 (0.0000)  time: 0.8638  data: 0.0629
🚀 Starting data_iter_step 201 (DataLoader iteration took 1.672s)...
Calculated step = 201, num_training_steps_per_epoch = 312
Global iteration it = 201
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 201...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.660s
🚀 Starting data_iter_step 202 (DataLoader iteration took 0.000s)...
Calculated step = 202, num_training_steps_per_epoch = 312
Global iteration it = 202
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 202...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.745s
🚀 Starting data_iter_step 203 (DataLoader iteration took 0.000s)...
Calculated step = 203, num_training_steps_per_epoch = 312
Global iteration it = 203
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 203...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.638s
🚀 Starting data_iter_step 204 (DataLoader iteration took 0.000s)...
Calculated step = 204, num_training_steps_per_epoch = 312
Global iteration it = 204
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 204...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.588s
🚀 Starting data_iter_step 205 (DataLoader iteration took 0.000s)...
Calculated step = 205, num_training_steps_per_epoch = 312
Global iteration it = 205
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 205...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.489s
🚀 Starting data_iter_step 206 (DataLoader iteration took 0.000s)...
Calculated step = 206, num_training_steps_per_epoch = 312
Global iteration it = 206
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 206...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.690s
🚀 Starting data_iter_step 207 (DataLoader iteration took 0.001s)...
Calculated step = 207, num_training_steps_per_epoch = 312
Global iteration it = 207
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 207...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.665s
🚀 Starting data_iter_step 208 (DataLoader iteration took 0.000s)...
Calculated step = 208, num_training_steps_per_epoch = 312
Global iteration it = 208
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 208...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.436s
🚀 Starting data_iter_step 209 (DataLoader iteration took 0.000s)...
Calculated step = 209, num_training_steps_per_epoch = 312
Global iteration it = 209
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 209...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 1.674s
🚀 Starting data_iter_step 210 (DataLoader iteration took 0.000s)...
Calculated step = 210, num_training_steps_per_epoch = 312
Global iteration it = 210
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 210...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.733s
Epoch: [0]  [210/312]  eta: 0:02:53  loss: 7.0312 (7.0417)  lr: 0.0000 (0.0000)  time: 0.8500  data: 0.0411
🚀 Starting data_iter_step 211 (DataLoader iteration took 0.001s)...
Calculated step = 211, num_training_steps_per_epoch = 312
Global iteration it = 211
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 211...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.000s: 6.90625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.227s
🔥 Total XLA step time: 0.521s
🚀 Starting data_iter_step 212 (DataLoader iteration took 0.000s)...
Calculated step = 212, num_training_steps_per_epoch = 312
Global iteration it = 212
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 212...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.011s, about to compute loss...
✅ Loss computed in 0.000s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.547s
🚀 Starting data_iter_step 213 (DataLoader iteration took 0.000s)...
Calculated step = 213, num_training_steps_per_epoch = 312
Global iteration it = 213
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 213...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.236s
🔥 Total XLA step time: 0.617s
🚀 Starting data_iter_step 214 (DataLoader iteration took 0.000s)...
Calculated step = 214, num_training_steps_per_epoch = 312
Global iteration it = 214
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 214...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.000s: 6.875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.655s
🚀 Starting data_iter_step 215 (DataLoader iteration took 0.000s)...
Calculated step = 215, num_training_steps_per_epoch = 312
Global iteration it = 215
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 215...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.700s
🚀 Starting data_iter_step 216 (DataLoader iteration took 0.230s)...
Calculated step = 216, num_training_steps_per_epoch = 312
Global iteration it = 216
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 216...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.623s
🚀 Starting data_iter_step 217 (DataLoader iteration took 0.000s)...
Calculated step = 217, num_training_steps_per_epoch = 312
Global iteration it = 217
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 217...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.224s
🔥 Total XLA step time: 1.508s
🚀 Starting data_iter_step 218 (DataLoader iteration took 0.000s)...
Calculated step = 218, num_training_steps_per_epoch = 312
Global iteration it = 218
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 218...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.606s
🚀 Starting data_iter_step 219 (DataLoader iteration took 0.000s)...
Calculated step = 219, num_training_steps_per_epoch = 312
Global iteration it = 219
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 219...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.546s
🚀 Starting data_iter_step 220 (DataLoader iteration took 0.000s)...
Calculated step = 220, num_training_steps_per_epoch = 312
Global iteration it = 220
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 220...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.694s
Epoch: [0]  [220/312]  eta: 0:02:32  loss: 7.0312 (7.0417)  lr: 0.0000 (0.0000)  time: 0.7310  data: 0.0117
🚀 Starting data_iter_step 221 (DataLoader iteration took 0.001s)...
Calculated step = 221, num_training_steps_per_epoch = 312
Global iteration it = 221
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 221...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.507s
🚀 Starting data_iter_step 222 (DataLoader iteration took 0.000s)...
Calculated step = 222, num_training_steps_per_epoch = 312
Global iteration it = 222
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 222...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.232s
🔥 Total XLA step time: 0.606s
🚀 Starting data_iter_step 223 (DataLoader iteration took 0.000s)...
Calculated step = 223, num_training_steps_per_epoch = 312
Global iteration it = 223
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 223...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.631s
🚀 Starting data_iter_step 224 (DataLoader iteration took 0.810s)...
Calculated step = 224, num_training_steps_per_epoch = 312
Global iteration it = 224
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 224...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.250s
🔥 Total XLA step time: 0.683s
🚀 Starting data_iter_step 225 (DataLoader iteration took 0.000s)...
Calculated step = 225, num_training_steps_per_epoch = 312
Global iteration it = 225
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 225...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 1.664s
🚀 Starting data_iter_step 226 (DataLoader iteration took 0.000s)...
Calculated step = 226, num_training_steps_per_epoch = 312
Global iteration it = 226
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 226...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.664s
🚀 Starting data_iter_step 227 (DataLoader iteration took 0.000s)...
Calculated step = 227, num_training_steps_per_epoch = 312
Global iteration it = 227
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 227...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.687s
🚀 Starting data_iter_step 228 (DataLoader iteration took 0.000s)...
Calculated step = 228, num_training_steps_per_epoch = 312
Global iteration it = 228
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 228...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.711s
🚀 Starting data_iter_step 229 (DataLoader iteration took 0.000s)...
Calculated step = 229, num_training_steps_per_epoch = 312
Global iteration it = 229
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 229...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.372s
🚀 Starting data_iter_step 230 (DataLoader iteration took 0.000s)...
Calculated step = 230, num_training_steps_per_epoch = 312
Global iteration it = 230
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 230...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.624s
Epoch: [0]  [230/312]  eta: 0:02:12  loss: 7.0312 (7.0417)  lr: 0.0000 (0.0000)  time: 0.7630  data: 0.0521
🚀 Starting data_iter_step 231 (DataLoader iteration took 0.001s)...
Calculated step = 231, num_training_steps_per_epoch = 312
Global iteration it = 231
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 231...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.247s
🔥 Total XLA step time: 0.626s
🚀 Starting data_iter_step 232 (DataLoader iteration took 0.451s)...
Calculated step = 232, num_training_steps_per_epoch = 312
Global iteration it = 232
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 232...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.020s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.298s
🚀 Starting data_iter_step 233 (DataLoader iteration took 0.000s)...
Calculated step = 233, num_training_steps_per_epoch = 312
Global iteration it = 233
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 233...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 2.074s
🚀 Starting data_iter_step 234 (DataLoader iteration took 0.000s)...
Calculated step = 234, num_training_steps_per_epoch = 312
Global iteration it = 234
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 234...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.234s
🔥 Total XLA step time: 0.636s
🚀 Starting data_iter_step 235 (DataLoader iteration took 0.000s)...
Calculated step = 235, num_training_steps_per_epoch = 312
Global iteration it = 235
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 235...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.626s
🚀 Starting data_iter_step 236 (DataLoader iteration took 0.000s)...
Calculated step = 236, num_training_steps_per_epoch = 312
Global iteration it = 236
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 236...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.740s
🚀 Starting data_iter_step 237 (DataLoader iteration took 0.000s)...
Calculated step = 237, num_training_steps_per_epoch = 312
Global iteration it = 237
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 237...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.563s
🚀 Starting data_iter_step 238 (DataLoader iteration took 0.000s)...
Calculated step = 238, num_training_steps_per_epoch = 312
Global iteration it = 238
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 238...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.614s
🚀 Starting data_iter_step 239 (DataLoader iteration took 0.000s)...
Calculated step = 239, num_training_steps_per_epoch = 312
Global iteration it = 239
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 239...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.599s
🚀 Starting data_iter_step 240 (DataLoader iteration took 0.318s)...
Calculated step = 240, num_training_steps_per_epoch = 312
Global iteration it = 240
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 240...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.647s
Epoch: [0]  [240/312]  eta: 0:01:54  loss: 7.0312 (7.0417)  lr: 0.0000 (0.0000)  time: 0.8105  data: 0.0791
🚀 Starting data_iter_step 241 (DataLoader iteration took 0.001s)...
Calculated step = 241, num_training_steps_per_epoch = 312
Global iteration it = 241
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 241...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.233s
🔥 Total XLA step time: 1.737s
🚀 Starting data_iter_step 242 (DataLoader iteration took 0.000s)...
Calculated step = 242, num_training_steps_per_epoch = 312
Global iteration it = 242
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 242...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.640s
🚀 Starting data_iter_step 243 (DataLoader iteration took 0.000s)...
Calculated step = 243, num_training_steps_per_epoch = 312
Global iteration it = 243
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 243...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.704s
🚀 Starting data_iter_step 244 (DataLoader iteration took 0.000s)...
Calculated step = 244, num_training_steps_per_epoch = 312
Global iteration it = 244
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 244...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.770s
🚀 Starting data_iter_step 245 (DataLoader iteration took 0.000s)...
Calculated step = 245, num_training_steps_per_epoch = 312
Global iteration it = 245
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 245...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.613s
🚀 Starting data_iter_step 246 (DataLoader iteration took 0.000s)...
Calculated step = 246, num_training_steps_per_epoch = 312
Global iteration it = 246
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 246...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.635s
🚀 Starting data_iter_step 247 (DataLoader iteration took 0.001s)...
Calculated step = 247, num_training_steps_per_epoch = 312
Global iteration it = 247
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 247...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.473s
🚀 Starting data_iter_step 248 (DataLoader iteration took 0.203s)...
Calculated step = 248, num_training_steps_per_epoch = 312
Global iteration it = 248
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 248...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.557s
🚀 Starting data_iter_step 249 (DataLoader iteration took 0.000s)...
Calculated step = 249, num_training_steps_per_epoch = 312
Global iteration it = 249
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 249...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.262s
🔥 Total XLA step time: 1.872s
🚀 Starting data_iter_step 250 (DataLoader iteration took 0.000s)...
Calculated step = 250, num_training_steps_per_epoch = 312
Global iteration it = 250
TPU mode: Processing iteration 250, step 250, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 250...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.624s
TPU mode: Iteration 250, TPU memory: {'bytes_used': 1608501760, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 0.121s
✅ Wandb logged in 0.000s
Epoch: [0]  [250/312]  eta: 0:01:36  loss: 7.0312 (7.0312)  lr: 0.0000 (0.0000)  time: 0.8601  data: 0.0487
🚀 Starting data_iter_step 251 (DataLoader iteration took 0.123s)...
Calculated step = 251, num_training_steps_per_epoch = 312
Global iteration it = 251
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 251...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.593s
🚀 Starting data_iter_step 252 (DataLoader iteration took 0.000s)...
Calculated step = 252, num_training_steps_per_epoch = 312
Global iteration it = 252
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 252...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.497s
🚀 Starting data_iter_step 253 (DataLoader iteration took 0.000s)...
Calculated step = 253, num_training_steps_per_epoch = 312
Global iteration it = 253
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 253...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.717s
🚀 Starting data_iter_step 254 (DataLoader iteration took 0.000s)...
Calculated step = 254, num_training_steps_per_epoch = 312
Global iteration it = 254
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 254...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.255s
🔥 Total XLA step time: 0.664s
🚀 Starting data_iter_step 255 (DataLoader iteration took 0.000s)...
Calculated step = 255, num_training_steps_per_epoch = 312
Global iteration it = 255
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 255...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.536s
🚀 Starting data_iter_step 256 (DataLoader iteration took 0.280s)...
Calculated step = 256, num_training_steps_per_epoch = 312
Global iteration it = 256
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 256...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.701s
🚀 Starting data_iter_step 257 (DataLoader iteration took 0.000s)...
Calculated step = 257, num_training_steps_per_epoch = 312
Global iteration it = 257
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 257...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 2.047s
🚀 Starting data_iter_step 258 (DataLoader iteration took 0.000s)...
Calculated step = 258, num_training_steps_per_epoch = 312
Global iteration it = 258
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 258...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.792s
🚀 Starting data_iter_step 259 (DataLoader iteration took 0.000s)...
Calculated step = 259, num_training_steps_per_epoch = 312
Global iteration it = 259
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 259...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.247s
🔥 Total XLA step time: 0.360s
🚀 Starting data_iter_step 260 (DataLoader iteration took 0.000s)...
Calculated step = 260, num_training_steps_per_epoch = 312
Global iteration it = 260
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 260...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.779s
Epoch: [0]  [260/312]  eta: 0:01:19  loss: 7.0312 (7.0312)  lr: 0.0000 (0.0000)  time: 0.8487  data: 0.0243
🚀 Starting data_iter_step 261 (DataLoader iteration took 0.001s)...
Calculated step = 261, num_training_steps_per_epoch = 312
Global iteration it = 261
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 261...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.221s
🔥 Total XLA step time: 0.446s
🚀 Starting data_iter_step 262 (DataLoader iteration took 0.000s)...
Calculated step = 262, num_training_steps_per_epoch = 312
Global iteration it = 262
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 262...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.247s
🔥 Total XLA step time: 0.638s
🚀 Starting data_iter_step 263 (DataLoader iteration took 0.000s)...
Calculated step = 263, num_training_steps_per_epoch = 312
Global iteration it = 263
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 263...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.538s
🚀 Starting data_iter_step 264 (DataLoader iteration took 0.549s)...
Calculated step = 264, num_training_steps_per_epoch = 312
Global iteration it = 264
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 264...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.250s
🔥 Total XLA step time: 0.303s
🚀 Starting data_iter_step 265 (DataLoader iteration took 0.000s)...
Calculated step = 265, num_training_steps_per_epoch = 312
Global iteration it = 265
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 265...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 2.243s
🚀 Starting data_iter_step 266 (DataLoader iteration took 0.000s)...
Calculated step = 266, num_training_steps_per_epoch = 312
Global iteration it = 266
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 266...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.607s
🚀 Starting data_iter_step 267 (DataLoader iteration took 0.000s)...
Calculated step = 267, num_training_steps_per_epoch = 312
Global iteration it = 267
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 267...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.662s
🚀 Starting data_iter_step 268 (DataLoader iteration took 0.000s)...
Calculated step = 268, num_training_steps_per_epoch = 312
Global iteration it = 268
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 268...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 0.636s
🚀 Starting data_iter_step 269 (DataLoader iteration took 0.000s)...
Calculated step = 269, num_training_steps_per_epoch = 312
Global iteration it = 269
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 269...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.435s
🚀 Starting data_iter_step 270 (DataLoader iteration took 0.000s)...
Calculated step = 270, num_training_steps_per_epoch = 312
Global iteration it = 270
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 270...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.665s
Epoch: [0]  [270/312]  eta: 0:01:03  loss: 7.0312 (7.0312)  lr: 0.0000 (0.0000)  time: 0.7874  data: 0.0416
🚀 Starting data_iter_step 271 (DataLoader iteration took 0.001s)...
Calculated step = 271, num_training_steps_per_epoch = 312
Global iteration it = 271
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 271...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.249s
🔥 Total XLA step time: 0.561s
🚀 Starting data_iter_step 272 (DataLoader iteration took 0.375s)...
Calculated step = 272, num_training_steps_per_epoch = 312
Global iteration it = 272
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 272...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.296s
🚀 Starting data_iter_step 273 (DataLoader iteration took 0.000s)...
Calculated step = 273, num_training_steps_per_epoch = 312
Global iteration it = 273
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 273...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 2.113s
🚀 Starting data_iter_step 274 (DataLoader iteration took 0.000s)...
Calculated step = 274, num_training_steps_per_epoch = 312
Global iteration it = 274
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 274...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.630s
🚀 Starting data_iter_step 275 (DataLoader iteration took 0.000s)...
Calculated step = 275, num_training_steps_per_epoch = 312
Global iteration it = 275
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 275...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.578s
🚀 Starting data_iter_step 276 (DataLoader iteration took 0.000s)...
Calculated step = 276, num_training_steps_per_epoch = 312
Global iteration it = 276
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 276...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.250s
🔥 Total XLA step time: 0.606s
🚀 Starting data_iter_step 277 (DataLoader iteration took 0.000s)...
Calculated step = 277, num_training_steps_per_epoch = 312
Global iteration it = 277
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 277...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.786s
🚀 Starting data_iter_step 278 (DataLoader iteration took 0.000s)...
Calculated step = 278, num_training_steps_per_epoch = 312
Global iteration it = 278
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 278...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.251s
🔥 Total XLA step time: 0.434s
🚀 Starting data_iter_step 279 (DataLoader iteration took 0.000s)...
Calculated step = 279, num_training_steps_per_epoch = 312
Global iteration it = 279
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 279...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.273s
🔥 Total XLA step time: 0.630s
🚀 Starting data_iter_step 280 (DataLoader iteration took 0.285s)...
Calculated step = 280, num_training_steps_per_epoch = 312
Global iteration it = 280
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 280...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.557s
Epoch: [0]  [280/312]  eta: 0:00:47  loss: 7.0312 (7.0312)  lr: 0.0000 (0.0000)  time: 0.7816  data: 0.0606
🚀 Starting data_iter_step 281 (DataLoader iteration took 0.001s)...
Calculated step = 281, num_training_steps_per_epoch = 312
Global iteration it = 281
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 281...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.239s
🔥 Total XLA step time: 1.752s
🚀 Starting data_iter_step 282 (DataLoader iteration took 0.000s)...
Calculated step = 282, num_training_steps_per_epoch = 312
Global iteration it = 282
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 282...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.638s
🚀 Starting data_iter_step 283 (DataLoader iteration took 0.000s)...
Calculated step = 283, num_training_steps_per_epoch = 312
Global iteration it = 283
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 283...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.247s
🔥 Total XLA step time: 0.626s
🚀 Starting data_iter_step 284 (DataLoader iteration took 0.000s)...
Calculated step = 284, num_training_steps_per_epoch = 312
Global iteration it = 284
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 284...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.241s
🔥 Total XLA step time: 0.644s
🚀 Starting data_iter_step 285 (DataLoader iteration took 0.000s)...
Calculated step = 285, num_training_steps_per_epoch = 312
Global iteration it = 285
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 285...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 0.632s
🚀 Starting data_iter_step 286 (DataLoader iteration took 0.000s)...
Calculated step = 286, num_training_steps_per_epoch = 312
Global iteration it = 286
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 286...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.627s
🚀 Starting data_iter_step 287 (DataLoader iteration took 0.000s)...
Calculated step = 287, num_training_steps_per_epoch = 312
Global iteration it = 287
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 287...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.84375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.650s
🚀 Starting data_iter_step 288 (DataLoader iteration took 0.361s)...
Calculated step = 288, num_training_steps_per_epoch = 312
Global iteration it = 288
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 288...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.291s
🚀 Starting data_iter_step 289 (DataLoader iteration took 0.000s)...
Calculated step = 289, num_training_steps_per_epoch = 312
Global iteration it = 289
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 289...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.245s
🔥 Total XLA step time: 1.979s
🚀 Starting data_iter_step 290 (DataLoader iteration took 0.000s)...
Calculated step = 290, num_training_steps_per_epoch = 312
Global iteration it = 290
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 290...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.017s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.364s
Epoch: [0]  [290/312]  eta: 0:00:31  loss: 7.0312 (7.0312)  lr: 0.0000 (0.0000)  time: 0.8237  data: 0.0512
🚀 Starting data_iter_step 291 (DataLoader iteration took 0.001s)...
Calculated step = 291, num_training_steps_per_epoch = 312
Global iteration it = 291
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 291...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.810s
🚀 Starting data_iter_step 292 (DataLoader iteration took 0.000s)...
Calculated step = 292, num_training_steps_per_epoch = 312
Global iteration it = 292
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 292...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.018s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.773s
🚀 Starting data_iter_step 293 (DataLoader iteration took 0.000s)...
Calculated step = 293, num_training_steps_per_epoch = 312
Global iteration it = 293
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 293...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.875, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.235s
🔥 Total XLA step time: 0.530s
🚀 Starting data_iter_step 294 (DataLoader iteration took 0.000s)...
Calculated step = 294, num_training_steps_per_epoch = 312
Global iteration it = 294
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 294...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.621s
🚀 Starting data_iter_step 295 (DataLoader iteration took 0.000s)...
Calculated step = 295, num_training_steps_per_epoch = 312
Global iteration it = 295
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 295...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.249s
🔥 Total XLA step time: 0.686s
🚀 Starting data_iter_step 296 (DataLoader iteration took 0.234s)...
Calculated step = 296, num_training_steps_per_epoch = 312
Global iteration it = 296
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 296...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.244s
🔥 Total XLA step time: 0.614s
🚀 Starting data_iter_step 297 (DataLoader iteration took 0.000s)...
Calculated step = 297, num_training_steps_per_epoch = 312
Global iteration it = 297
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 297...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 1.794s
🚀 Starting data_iter_step 298 (DataLoader iteration took 0.000s)...
Calculated step = 298, num_training_steps_per_epoch = 312
Global iteration it = 298
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 298...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.246s
🔥 Total XLA step time: 0.405s
🚀 Starting data_iter_step 299 (DataLoader iteration took 0.000s)...
Calculated step = 299, num_training_steps_per_epoch = 312
Global iteration it = 299
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 299...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.721s
🚀 Starting data_iter_step 300 (DataLoader iteration took 0.000s)...
Calculated step = 300, num_training_steps_per_epoch = 312
Global iteration it = 300
TPU mode: Processing iteration 300, step 300, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 300...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.248s
🔥 Total XLA step time: 0.502s
TPU mode: Iteration 300, TPU memory: {'bytes_used': 1618133504, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 0.310s
✅ Wandb logged in 0.000s
Epoch: [0]  [300/312]  eta: 0:00:17  loss: 7.0312 (7.0227)  lr: 0.0000 (0.0001)  time: 0.8311  data: 0.0299
🚀 Starting data_iter_step 301 (DataLoader iteration took 0.312s)...
Calculated step = 301, num_training_steps_per_epoch = 312
Global iteration it = 301
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 301...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.278s
🔥 Total XLA step time: 0.720s
🚀 Starting data_iter_step 302 (DataLoader iteration took 0.000s)...
Calculated step = 302, num_training_steps_per_epoch = 312
Global iteration it = 302
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 302...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.238s
🔥 Total XLA step time: 0.624s
🚀 Starting data_iter_step 303 (DataLoader iteration took 0.000s)...
Calculated step = 303, num_training_steps_per_epoch = 312
Global iteration it = 303
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 303...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.240s
🔥 Total XLA step time: 0.781s
🚀 Starting data_iter_step 304 (DataLoader iteration took 0.000s)...
Calculated step = 304, num_training_steps_per_epoch = 312
Global iteration it = 304
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 304...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.190s
🔥 Total XLA step time: 0.656s
🚀 Starting data_iter_step 305 (DataLoader iteration took 0.000s)...
Calculated step = 305, num_training_steps_per_epoch = 312
Global iteration it = 305
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 305...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.189s
🔥 Total XLA step time: 2.005s
🚀 Starting data_iter_step 306 (DataLoader iteration took 0.000s)...
Calculated step = 306, num_training_steps_per_epoch = 312
Global iteration it = 306
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 306...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.189s
🔥 Total XLA step time: 0.565s
🚀 Starting data_iter_step 307 (DataLoader iteration took 0.000s)...
Calculated step = 307, num_training_steps_per_epoch = 312
Global iteration it = 307
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 307...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.011s, about to compute loss...
✅ Loss computed in 0.001s: 6.9375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.190s
🔥 Total XLA step time: 0.662s
🚀 Starting data_iter_step 308 (DataLoader iteration took 0.000s)...
Calculated step = 308, num_training_steps_per_epoch = 312
Global iteration it = 308
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 308...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.190s
🔥 Total XLA step time: 0.584s
🚀 Starting data_iter_step 309 (DataLoader iteration took 0.000s)...
Calculated step = 309, num_training_steps_per_epoch = 312
Global iteration it = 309
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 309...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.011s, about to compute loss...
✅ Loss computed in 0.000s: 6.9375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.189s
🔥 Total XLA step time: 0.488s
🚀 Starting data_iter_step 310 (DataLoader iteration took 0.000s)...
Calculated step = 310, num_training_steps_per_epoch = 312
Global iteration it = 310
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 310...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.011s, about to compute loss...
✅ Loss computed in 0.000s: 6.9375, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.189s
🔥 Total XLA step time: 0.517s
Epoch: [0]  [310/312]  eta: 0:00:02  loss: 7.0312 (7.0227)  lr: 0.0000 (0.0001)  time: 0.7829  data: 0.0119
🚀 Starting data_iter_step 311 (DataLoader iteration took 0.001s)...
Calculated step = 311, num_training_steps_per_epoch = 312
Global iteration it = 311
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 311...
About to apply mixup if present...
About to enter torch_xla.step() context...
✅ Data already on XLA device: xla:0
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 6.90625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.189s
🔥 Total XLA step time: 0.548s
Epoch: [0]  [311/312]  eta: 0:00:01  loss: 7.0312 (7.0227)  lr: 0.0000 (0.0001)  time: 0.7698  data: 0.0119
Epoch: [0] Total time: 0:07:19 (1.4085 s / it)
Averaged stats: loss: 7.0312 (7.0000)  lr: 0.0000 (0.0001)
Done train_one_epoch...
