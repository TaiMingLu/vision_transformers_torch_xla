W&B online run started. URL: https://wandb.ai/ttl/tpu-vit/runs/u1p4c567
Mixup is activated!
WARNING:timm.models._builder:No pretrained configuration specified for my_vit_b model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Forcing XLA synchronization after model.to(device)...
XLA mark_step() after model.to(device) completed
Forcing XLA synchronization before parameter counting...
XLA mark_step() completed
TPU spawned process: world_size = 32
Calculated total_batch_size = 4096
Number of training examples = 1281167
LR = 0.00400000
Batch size = 4096
Update frequent = 1
Number of training steps per epoch = 312
TPU multihost mode: XLA handles data distribution internally
About to create optimizer...
TPU mode: Using name-only parameter grouping to avoid XLA compilation
TPU mode: Collecting parameter names...
TPU mode: Found 152 trainable parameters
TPU mode: Grouping parameters by names...
Processing parameter 1/152: cls_token
Processing parameter 51/152: blocks.3.mlp.fc2.weight
Processing parameter 101/152: blocks.8.norm1.weight
Processing parameter 151/152: head.weight
TPU mode: Parameter names grouped successfully
TPU mode: Mapping names to tensors...
TPU mode: All 152 parameters mapped successfully
Parameter iteration completed! Processed 152 parameters.
TPU mode: Created 2 parameter groups, returning optimizer groups...
Parameter groups created successfully
Optimizer created successfully
TPU mode: Skipping loss scaler (XLA handles mixed precision)
TPU mode: About to set loss_scaler = None...
TPU mode: Loss scaler set to None
About to create Cosine LR scheduler...
Set warmup steps = 6240
LR scheduler created successfully
About to continue to weight decay scheduler...
About to check args.weight_decay_end...
Directly setting weight_decay_end to avoid freeze...
weight_decay_end set to: 0.05
About to call cosine_scheduler for weight decay...
Calling with: wd=0.05, wd_end=0.05, epochs=100, steps_per_epoch=312
Set warmup steps = 0
cosine_scheduler completed, about to calculate min/max...
Weight decay schedule created: 0.05 -> 0.05
About to create criterion...
Created SoftTargetCrossEntropy criterion
criterion = SoftTargetCrossEntropy()
TPU mode: Skipping warmup (caused shape errors), will compile on first iteration
About to call auto_load_model...
auto_load_model completed
About to define get_eval_model helper function...
get_eval_model function defined
About to set max_accuracy...
max_accuracy initialized
Start training for 100 epochs
About to record start_time...
About to enter training loop...
Starting epoch 0...
Setting sampler epoch...
Setting wandb steps...
About to call train_one_epoch...
train_one_epoch started: epoch=0, tpu=True
About to zero_grad optimizer...
About to start data loader loop...
🚀 Starting data_iter_step 0 (DataLoader iteration took 0.000s)...
Calculated step = 0, num_training_steps_per_epoch = 312
Global iteration it = 0
TPU mode: Processing iteration 0, step 0, samples shape: torch.Size([128, 3, 224, 224])
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 0...
TPU mode: Current device = xla:0, device type = xla
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
TPU mode: samples.device = xla:0
TPU mode: targets.device = xla:0
TPU mode: model device = xla:0
About to call model forward...
✅ Model forward completed in 0.020s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.023s
About to run optimizer step...
✅ Optimizer step completed in 26.560s
🔥 Total XLA step time: 38.185s
TPU mode: Iteration 0 completed in 38.19s
📝 End of iteration 0 - about to continue loop...
TPU mode: Iteration 0, TPU memory: {'bytes_used': 1119007744, 'bytes_limit': 33550237696}
🔧 Starting metric updates...
✅ Metric logger updated in 18.740s
✅ Wandb logged in 0.000s
Epoch: [0]  [  0/312]  eta: 4:59:53  loss: 7.0938 (7.0938)  lr: 0.0000 (0.0000)  time: 57.6728  data: 0.7428
🚀 Starting data_iter_step 1 (DataLoader iteration took 34.613s)...
Calculated step = 1, num_training_steps_per_epoch = 312
Global iteration it = 1
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 1...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.012s
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.000s: 7.09375, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 28.995s
🔥 Total XLA step time: 29.156s
TPU mode: Iteration 1 completed in 29.21s
📝 End of iteration 1 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 10.533s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 2 (DataLoader iteration took 26.498s)...
Calculated step = 2, num_training_steps_per_epoch = 312
Global iteration it = 2
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 2...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.007s
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.15625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.183s
🔥 Total XLA step time: 0.238s
TPU mode: Iteration 2 completed in 0.25s
📝 End of iteration 2 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.213s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 3 (DataLoader iteration took 15.955s)...
Calculated step = 3, num_training_steps_per_epoch = 312
Global iteration it = 3
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 3...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.008s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.183s
🔥 Total XLA step time: 0.240s
TPU mode: Iteration 3 completed in 0.25s
📝 End of iteration 3 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.227s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 4 (DataLoader iteration took 15.830s)...
Calculated step = 4, num_training_steps_per_epoch = 312
Global iteration it = 4
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 4...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.022s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.183s
🔥 Total XLA step time: 0.257s
TPU mode: Iteration 4 completed in 0.34s
📝 End of iteration 4 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.161s
✅ Wandb logged in 0.000s
🚀 Starting data_iter_step 5 (DataLoader iteration took 15.679s)...
Calculated step = 5, num_training_steps_per_epoch = 312
Global iteration it = 5
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 5...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.033s
About to call model forward...
✅ Model forward completed in 0.023s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.021s
About to run optimizer step...
✅ Optimizer step completed in 0.187s
🔥 Total XLA step time: 0.305s
TPU mode: Iteration 5 completed in 0.47s
📝 End of iteration 5 - about to continue loop...
✅ Wandb logged in 0.236s
🚀 Starting data_iter_step 6 (DataLoader iteration took 15.831s)...
Calculated step = 6, num_training_steps_per_epoch = 312
Global iteration it = 6
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 6...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.006s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.217s
🔥 Total XLA step time: 0.269s
TPU mode: Iteration 6 completed in 0.28s
📝 End of iteration 6 - about to continue loop...
✅ Wandb logged in 0.091s
🚀 Starting data_iter_step 7 (DataLoader iteration took 15.407s)...
Calculated step = 7, num_training_steps_per_epoch = 312
Global iteration it = 7
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 7...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.030s
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.000s: 7.125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.189s
🔥 Total XLA step time: 0.276s
TPU mode: Iteration 7 completed in 0.39s
📝 End of iteration 7 - about to continue loop...
✅ Wandb logged in 0.321s
🚀 Starting data_iter_step 8 (DataLoader iteration took 15.892s)...
Calculated step = 8, num_training_steps_per_epoch = 312
Global iteration it = 8
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 8...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.040s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 7.09375, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.184s
🔥 Total XLA step time: 0.282s
TPU mode: Iteration 8 completed in 0.40s
📝 End of iteration 8 - about to continue loop...
✅ Wandb logged in 0.086s
🚀 Starting data_iter_step 9 (DataLoader iteration took 15.590s)...
Calculated step = 9, num_training_steps_per_epoch = 312
Global iteration it = 9
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 9...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.027s
About to call model forward...
✅ Model forward completed in 0.020s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.189s
🔥 Total XLA step time: 0.295s
TPU mode: Iteration 9 completed in 0.44s
📝 End of iteration 9 - about to continue loop...
✅ Wandb logged in 0.083s
🚀 Starting data_iter_step 10 (DataLoader iteration took 11.270s)...
Calculated step = 10, num_training_steps_per_epoch = 312
Global iteration it = 10
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 10...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.184s
🔥 Total XLA step time: 0.232s
TPU mode: Iteration 10 completed in 0.23s
📝 End of iteration 10 - about to continue loop...
🔧 Starting metric updates...
✅ Metric logger updated in 0.932s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 10/312]  eta: 1:56:33  loss: 7.0938 (7.0729)  lr: 0.0000 (0.0000)  time: 23.1563  data: 13.8731
🚀 Starting data_iter_step 11 (DataLoader iteration took 16.862s)...
Calculated step = 11, num_training_steps_per_epoch = 312
Global iteration it = 11
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 11...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.188s
🔥 Total XLA step time: 0.243s
TPU mode: Iteration 11 completed in 0.25s
📝 End of iteration 11 - about to continue loop...
✅ Wandb logged in 0.188s
🚀 Starting data_iter_step 12 (DataLoader iteration took 15.776s)...
Calculated step = 12, num_training_steps_per_epoch = 312
Global iteration it = 12
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 12...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.012s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.09375, about to backward...
✅ Backward completed in 0.023s
About to run optimizer step...
✅ Optimizer step completed in 0.213s
🔥 Total XLA step time: 0.289s
TPU mode: Iteration 12 completed in 0.39s
📝 End of iteration 12 - about to continue loop...
✅ Wandb logged in 0.098s
🚀 Starting data_iter_step 13 (DataLoader iteration took 15.622s)...
Calculated step = 13, num_training_steps_per_epoch = 312
Global iteration it = 13
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 13...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.013s
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.000s: 7.09375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.184s
🔥 Total XLA step time: 0.259s
TPU mode: Iteration 13 completed in 0.35s
📝 End of iteration 13 - about to continue loop...
✅ Wandb logged in 0.081s
🚀 Starting data_iter_step 14 (DataLoader iteration took 15.844s)...
Calculated step = 14, num_training_steps_per_epoch = 312
Global iteration it = 14
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 14...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.034s
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.022s
About to run optimizer step...
✅ Optimizer step completed in 0.218s
🔥 Total XLA step time: 0.327s
TPU mode: Iteration 14 completed in 0.45s
📝 End of iteration 14 - about to continue loop...
✅ Wandb logged in 0.081s
🚀 Starting data_iter_step 15 (DataLoader iteration took 15.479s)...
Calculated step = 15, num_training_steps_per_epoch = 312
Global iteration it = 15
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 15...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.032s
About to call model forward...
✅ Model forward completed in 0.020s, about to compute loss...
✅ Loss computed in 0.000s: 7.09375, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.187s
🔥 Total XLA step time: 0.284s
TPU mode: Iteration 15 completed in 0.45s
📝 End of iteration 15 - about to continue loop...
✅ Wandb logged in 0.087s
🚀 Starting data_iter_step 16 (DataLoader iteration took 15.575s)...
Calculated step = 16, num_training_steps_per_epoch = 312
Global iteration it = 16
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 16...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.035s
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.187s
🔥 Total XLA step time: 0.280s
TPU mode: Iteration 16 completed in 0.43s
📝 End of iteration 16 - about to continue loop...
✅ Wandb logged in 0.084s
🚀 Starting data_iter_step 17 (DataLoader iteration took 15.610s)...
Calculated step = 17, num_training_steps_per_epoch = 312
Global iteration it = 17
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 17...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.030s
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.022s
About to run optimizer step...
✅ Optimizer step completed in 0.242s
🔥 Total XLA step time: 0.341s
TPU mode: Iteration 17 completed in 0.45s
📝 End of iteration 17 - about to continue loop...
✅ Wandb logged in 0.086s
🚀 Starting data_iter_step 18 (DataLoader iteration took 15.272s)...
Calculated step = 18, num_training_steps_per_epoch = 312
Global iteration it = 18
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 18...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.027s
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.204s
🔥 Total XLA step time: 0.289s
TPU mode: Iteration 18 completed in 0.40s
📝 End of iteration 18 - about to continue loop...
✅ Wandb logged in 0.083s
🚀 Starting data_iter_step 19 (DataLoader iteration took 15.218s)...
Calculated step = 19, num_training_steps_per_epoch = 312
Global iteration it = 19
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 19...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.023s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.202s
🔥 Total XLA step time: 0.282s
TPU mode: Iteration 19 completed in 0.40s
📝 End of iteration 19 - about to continue loop...
✅ Wandb logged in 0.078s
🚀 Starting data_iter_step 20 (DataLoader iteration took 12.005s)...
Calculated step = 20, num_training_steps_per_epoch = 312
Global iteration it = 20
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 20...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.185s
🔥 Total XLA step time: 0.234s
🔧 Starting metric updates...
✅ Metric logger updated in 0.080s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 20/312]  eta: 1:35:14  loss: 7.0938 (7.0670)  lr: 0.0000 (0.0000)  time: 17.6642  data: 15.1661
🚀 Starting data_iter_step 21 (DataLoader iteration took 27.981s)...
Calculated step = 21, num_training_steps_per_epoch = 312
Global iteration it = 21
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 21...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.036s
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.182s
🔥 Total XLA step time: 0.272s
✅ Wandb logged in 0.331s
🚀 Starting data_iter_step 22 (DataLoader iteration took 43.895s)...
Calculated step = 22, num_training_steps_per_epoch = 312
Global iteration it = 22
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 22...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.183s
🔥 Total XLA step time: 0.230s
✅ Wandb logged in 0.693s
🚀 Starting data_iter_step 23 (DataLoader iteration took 58.694s)...
Calculated step = 23, num_training_steps_per_epoch = 312
Global iteration it = 23
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 23...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.004s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 7.09375, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.269s
🔥 Total XLA step time: 0.321s
✅ Wandb logged in 1.792s
🚀 Starting data_iter_step 24 (DataLoader iteration took 75.967s)...
Calculated step = 24, num_training_steps_per_epoch = 312
Global iteration it = 24
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 24...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.026s
About to call model forward...
✅ Model forward completed in 0.034s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.026s
About to run optimizer step...
✅ Optimizer step completed in 0.243s
🔥 Total XLA step time: 0.366s
✅ Wandb logged in 0.751s
🚀 Starting data_iter_step 25 (DataLoader iteration took 92.302s)...
Calculated step = 25, num_training_steps_per_epoch = 312
Global iteration it = 25
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 25...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.049s
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.003s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.212s
🔥 Total XLA step time: 0.329s
✅ Wandb logged in 0.147s
🚀 Starting data_iter_step 26 (DataLoader iteration took 107.892s)...
Calculated step = 26, num_training_steps_per_epoch = 312
Global iteration it = 26
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 26...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.028s
About to call model forward...
✅ Model forward completed in 0.020s, about to compute loss...
✅ Loss computed in 0.000s: 6.96875, about to backward...
✅ Backward completed in 0.019s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.288s
✅ Wandb logged in 0.444s
🚀 Starting data_iter_step 27 (DataLoader iteration took 123.886s)...
Calculated step = 27, num_training_steps_per_epoch = 312
Global iteration it = 27
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 27...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.028s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.185s
🔥 Total XLA step time: 0.271s
✅ Wandb logged in 0.320s
🚀 Starting data_iter_step 28 (DataLoader iteration took 139.861s)...
Calculated step = 28, num_training_steps_per_epoch = 312
Global iteration it = 28
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 28...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.036s
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.001s: 6.96875, about to backward...
✅ Backward completed in 0.016s
About to run optimizer step...
✅ Optimizer step completed in 0.207s
🔥 Total XLA step time: 0.297s
✅ Wandb logged in 0.317s
🚀 Starting data_iter_step 29 (DataLoader iteration took 155.979s)...
Calculated step = 29, num_training_steps_per_epoch = 312
Global iteration it = 29
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 29...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.029s
About to call model forward...
✅ Model forward completed in 0.024s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.276s
✅ Wandb logged in 0.087s
🚀 Starting data_iter_step 30 (DataLoader iteration took 164.232s)...
Calculated step = 30, num_training_steps_per_epoch = 312
Global iteration it = 30
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 30...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.183s
🔥 Total XLA step time: 0.231s
🔧 Starting metric updates...
✅ Metric logger updated in 3.244s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 30/312]  eta: 1:25:51  loss: 7.0312 (7.0625)  lr: 0.0000 (0.0000)  time: 15.5813  data: 14.7456
🚀 Starting data_iter_step 31 (DataLoader iteration took 183.242s)...
Calculated step = 31, num_training_steps_per_epoch = 312
Global iteration it = 31
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 31...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.184s
🔥 Total XLA step time: 0.237s
✅ Wandb logged in 0.476s
🚀 Starting data_iter_step 32 (DataLoader iteration took 198.817s)...
Calculated step = 32, num_training_steps_per_epoch = 312
Global iteration it = 32
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 32...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.038s
About to call model forward...
✅ Model forward completed in 0.025s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.025s
About to run optimizer step...
✅ Optimizer step completed in 0.256s
🔥 Total XLA step time: 0.370s
✅ Wandb logged in 0.677s
🚀 Starting data_iter_step 33 (DataLoader iteration took 214.970s)...
Calculated step = 33, num_training_steps_per_epoch = 312
Global iteration it = 33
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 33...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.027s
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.015s
About to run optimizer step...
✅ Optimizer step completed in 0.191s
🔥 Total XLA step time: 0.276s
✅ Wandb logged in 0.353s
🚀 Starting data_iter_step 34 (DataLoader iteration took 231.019s)...
Calculated step = 34, num_training_steps_per_epoch = 312
Global iteration it = 34
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 34...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.029s
About to call model forward...
✅ Model forward completed in 0.015s, about to compute loss...
✅ Loss computed in 0.000s: 7.03125, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.195s
🔥 Total XLA step time: 0.285s
✅ Wandb logged in 0.574s
🚀 Starting data_iter_step 35 (DataLoader iteration took 247.402s)...
Calculated step = 35, num_training_steps_per_epoch = 312
Global iteration it = 35
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 35...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.034s
About to call model forward...
✅ Model forward completed in 0.014s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.025s
About to run optimizer step...
✅ Optimizer step completed in 0.204s
🔥 Total XLA step time: 0.308s
✅ Wandb logged in 0.082s
🚀 Starting data_iter_step 36 (DataLoader iteration took 263.089s)...
Calculated step = 36, num_training_steps_per_epoch = 312
Global iteration it = 36
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 36...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.035s
About to call model forward...
✅ Model forward completed in 0.019s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.014s
About to run optimizer step...
✅ Optimizer step completed in 0.192s
🔥 Total XLA step time: 0.285s
✅ Wandb logged in 0.082s
🚀 Starting data_iter_step 37 (DataLoader iteration took 276.729s)...
Calculated step = 37, num_training_steps_per_epoch = 312
Global iteration it = 37
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 37...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.002s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 7.0, about to backward...
✅ Backward completed in 0.018s
About to run optimizer step...
✅ Optimizer step completed in 0.281s
🔥 Total XLA step time: 0.345s
✅ Wandb logged in 2.020s
🚀 Starting data_iter_step 38 (DataLoader iteration took 294.351s)...
Calculated step = 38, num_training_steps_per_epoch = 312
Global iteration it = 38
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 38...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.031s
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.213s
🔥 Total XLA step time: 0.304s
✅ Wandb logged in 0.183s
🚀 Starting data_iter_step 39 (DataLoader iteration took 310.407s)...
Calculated step = 39, num_training_steps_per_epoch = 312
Global iteration it = 39
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 39...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.034s
About to call model forward...
✅ Model forward completed in 0.019s, about to compute loss...
✅ Loss computed in 0.000s: 7.03125, about to backward...
✅ Backward completed in 0.013s
About to run optimizer step...
✅ Optimizer step completed in 0.205s
🔥 Total XLA step time: 0.299s
✅ Wandb logged in 0.080s
🚀 Starting data_iter_step 40 (DataLoader iteration took 321.534s)...
Calculated step = 40, num_training_steps_per_epoch = 312
Global iteration it = 40
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 40...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.185s
🔥 Total XLA step time: 0.233s
🔧 Starting metric updates...
✅ Metric logger updated in 0.080s
✅ Wandb logged in 0.000s
Epoch: [0]  [ 40/312]  eta: 1:19:39  loss: 7.0312 (7.0590)  lr: 0.0000 (0.0000)  time: 15.4764  data: 14.4547
🚀 Starting data_iter_step 41 (DataLoader iteration took 337.746s)...
Calculated step = 41, num_training_steps_per_epoch = 312
Global iteration it = 41
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 41...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.004s
About to call model forward...
✅ Model forward completed in 0.013s, about to compute loss...
✅ Loss computed in 0.001s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.184s
🔥 Total XLA step time: 0.238s
✅ Wandb logged in 0.293s
🚀 Starting data_iter_step 42 (DataLoader iteration took 353.465s)...
Calculated step = 42, num_training_steps_per_epoch = 312
Global iteration it = 42
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 42...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.002s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.186s
🔥 Total XLA step time: 0.235s
✅ Wandb logged in 0.580s
🚀 Starting data_iter_step 43 (DataLoader iteration took 369.026s)...
Calculated step = 43, num_training_steps_per_epoch = 312
Global iteration it = 43
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 43...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.001s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.001s: 7.0, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.183s
🔥 Total XLA step time: 0.232s
✅ Wandb logged in 0.953s
🚀 Starting data_iter_step 44 (DataLoader iteration took 383.973s)...
Calculated step = 44, num_training_steps_per_epoch = 312
Global iteration it = 44
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 44...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.013s
About to call model forward...
✅ Model forward completed in 0.012s, about to compute loss...
✅ Loss computed in 0.000s: 7.09375, about to backward...
✅ Backward completed in 0.011s
About to run optimizer step...
✅ Optimizer step completed in 0.192s
🔥 Total XLA step time: 0.259s
✅ Wandb logged in 1.799s
🚀 Starting data_iter_step 45 (DataLoader iteration took 401.472s)...
Calculated step = 45, num_training_steps_per_epoch = 312
Global iteration it = 45
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 45...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.039s
About to call model forward...
✅ Model forward completed in 0.017s, about to compute loss...
✅ Loss computed in 0.001s: 7.15625, about to backward...
✅ Backward completed in 0.021s
About to run optimizer step...
✅ Optimizer step completed in 0.191s
🔥 Total XLA step time: 0.301s
✅ Wandb logged in 0.265s
🚀 Starting data_iter_step 46 (DataLoader iteration took 416.964s)...
Calculated step = 46, num_training_steps_per_epoch = 312
Global iteration it = 46
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 46...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.020s
About to call model forward...
✅ Model forward completed in 0.018s, about to compute loss...
✅ Loss computed in 0.000s: 7.0, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.184s
🔥 Total XLA step time: 0.263s
✅ Wandb logged in 0.725s
🚀 Starting data_iter_step 47 (DataLoader iteration took 431.339s)...
Calculated step = 47, num_training_steps_per_epoch = 312
Global iteration it = 47
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 47...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.036s
About to call model forward...
✅ Model forward completed in 0.019s, about to compute loss...
✅ Loss computed in 0.000s: 7.0625, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.195s
🔥 Total XLA step time: 0.290s
✅ Wandb logged in 1.926s
🚀 Starting data_iter_step 48 (DataLoader iteration took 448.554s)...
Calculated step = 48, num_training_steps_per_epoch = 312
Global iteration it = 48
About to update LR/WD schedules...
✅ LR/WD schedule update completed in 0.000s
About to enter forward/backward section, use_amp=False, tpu=True...
Taking TPU path...
TPU mode: Starting XLA step for iteration 48...
About to apply mixup if present...
About to enter torch_xla.step() context...
Inside torch_xla.step(), moving data to device...
✅ Data moved to device in 0.049s
About to call model forward...
✅ Model forward completed in 0.016s, about to compute loss...
✅ Loss computed in 0.000s: 7.03125, about to backward...
✅ Backward completed in 0.012s
About to run optimizer step...
✅ Optimizer step completed in 0.187s
🔥 Total XLA step time: 0.298s
✅ Wandb logged in 1.017s
🚀 Starting data_iter_step 49 (DataLoader iteration took 465.287s)...
Calculated step = 49, num_training_steps_per_epoch = 312
